{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSO7A3t5whWk",
        "outputId": "fadbc648-628c-42ba-fc93-e418c276ed1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#Import statements\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "#Now we load scikit-learn datasets and functionalities\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Eliminate randomness\n",
        "torch.manual_seed(42)\n",
        "\n",
        "#Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "print(X[0])\n",
        "print('***********')\n",
        "print(y[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDube0SAxwdk",
        "outputId": "c208100c-348f-4b9b-c6d6-bade3735792a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.1 3.5 1.4 0.2]\n",
            "***********\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic train/test split and feature scaling\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train, X_test, y_t, y_te = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_t = scaler.fit_transform(X_train)\n",
        "X_te = scaler.transform(X_test)\n",
        "\n",
        "#Conversion from numpy array to PyTorch tensor\n",
        "#Note that data types need to be different for features (float due to gradients) and labels (loss functions need lookup)\n",
        "X_t = torch.tensor(X_t, dtype=torch.float32)\n",
        "y_t = torch.tensor(y_t, dtype=torch.long)\n",
        "X_te = torch.tensor(X_te, dtype=torch.float32)\n",
        "y_te = torch.tensor(y_te, dtype=torch.long)"
      ],
      "metadata": {
        "id": "VlSod1W9x7w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we create the TensorDataset (data container) and DataLoader (magic wrapper that simplifies working with data)\n",
        "#Note: sometimes you want TensorDataset (most things, your data can fit on RAM), but sometimes you want Dataset (large datasets, like 50k medical images)\n",
        "\n",
        "train_dataset = TensorDataset(X_t, y_t)\n",
        "test_dataset = TensorDataset(X_te, y_te)\n",
        "\n",
        "batch = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch, shuffle=False)"
      ],
      "metadata": {
        "id": "YDdjFKK3y-Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the model\n",
        "#Note! nn.Module isn't your basic parent class that gives a few helper methods. It's a registry for parameters & auto-gradients, and a whole module tracking system\n",
        "#Thus, we need to call that second \"init\" function to set all of that up\n",
        "\n",
        "class IrisModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # __init__ defines the layers/components (the 'what') of the neural network.\n",
        "    # It's setting up the building blocks.\n",
        "    self.fc1 = nn.Linear(4,32) #4 input features and 32 neurons\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(32,16) #32 input features (sizes must match!) and 16 output feature\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(16,3) #Need 3 final logits for the multi-class classification\n",
        "\n",
        "  def forward(self, x):\n",
        "    # forward defines the computational graph (the 'how') using the components\n",
        "    # defined in __init__. It specifies the data flow through the network.\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu2(x)\n",
        "    logits = self.fc3(x)\n",
        "\n",
        "    return logits"
      ],
      "metadata": {
        "id": "GdBt4VYH309u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T5ipRevOJgO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = IrisModel().to(device) #Move model to GPU/CPU\n",
        "\n",
        "criteria = nn.CrossEntropyLoss() #Define our loss function\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.01) #Define our optimizer"
      ],
      "metadata": {
        "id": "RvHJyxX3ralm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#And now we define our training loop\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train() #Set model to training mode\n",
        "  total_loss = 0 #Here is where we accumulate the loss so we can track it on a per-epoch basis\n",
        "\n",
        "  for features, labels in train_loader: #Here's where the magic of DataLoader comes into play: it helps organize our data into something sensible (this loop goes over all of the batches)\n",
        "\n",
        "    #Move tensors to the configured device\n",
        "    features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "    optimizer.zero_grad() #We actually have to clear the gradient between runs - this is an artifact of when hardware usually couldn't handle a whole dataset\n",
        "\n",
        "    outputs = model(features) #Calculate the outputs for the current batch (forward pass)\n",
        "\n",
        "    loss = criteria(outputs, labels) #Compute your loss function\n",
        "\n",
        "    loss.backward() #Backpropagation step\n",
        "\n",
        "    optimizer.step() #Update the weights\n",
        "\n",
        "    total_loss += loss.item() #Add this batch's loss\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfSz46iirskn",
        "outputId": "9cc67ec3-e338-4a36-c267-9e1a1ed19c4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 1.1081\n",
            "Epoch [1/100], Loss: 2.1554\n",
            "Epoch [1/100], Loss: 3.1583\n",
            "Epoch [1/100], Loss: 4.0441\n",
            "Epoch [1/100], Loss: 4.8994\n",
            "Epoch [1/100], Loss: 5.8020\n",
            "Epoch [1/100], Loss: 6.5192\n",
            "Epoch [1/100], Loss: 7.1100\n",
            "Epoch [2/100], Loss: 0.8048\n",
            "Epoch [2/100], Loss: 1.3643\n",
            "Epoch [2/100], Loss: 1.8320\n",
            "Epoch [2/100], Loss: 2.2840\n",
            "Epoch [2/100], Loss: 2.8510\n",
            "Epoch [2/100], Loss: 3.2603\n",
            "Epoch [2/100], Loss: 3.8055\n",
            "Epoch [2/100], Loss: 4.1613\n",
            "Epoch [3/100], Loss: 0.4756\n",
            "Epoch [3/100], Loss: 0.9779\n",
            "Epoch [3/100], Loss: 1.2264\n",
            "Epoch [3/100], Loss: 1.5047\n",
            "Epoch [3/100], Loss: 1.7961\n",
            "Epoch [3/100], Loss: 2.0987\n",
            "Epoch [3/100], Loss: 2.4521\n",
            "Epoch [3/100], Loss: 2.7225\n",
            "Epoch [4/100], Loss: 0.2061\n",
            "Epoch [4/100], Loss: 0.3930\n",
            "Epoch [4/100], Loss: 0.7097\n",
            "Epoch [4/100], Loss: 0.9124\n",
            "Epoch [4/100], Loss: 1.2074\n",
            "Epoch [4/100], Loss: 1.2996\n",
            "Epoch [4/100], Loss: 1.5142\n",
            "Epoch [4/100], Loss: 1.7036\n",
            "Epoch [5/100], Loss: 0.1168\n",
            "Epoch [5/100], Loss: 0.2064\n",
            "Epoch [5/100], Loss: 0.6072\n",
            "Epoch [5/100], Loss: 0.8446\n",
            "Epoch [5/100], Loss: 0.9136\n",
            "Epoch [5/100], Loss: 1.0699\n",
            "Epoch [5/100], Loss: 1.1817\n",
            "Epoch [5/100], Loss: 1.2747\n",
            "Epoch [6/100], Loss: 0.1407\n",
            "Epoch [6/100], Loss: 0.3284\n",
            "Epoch [6/100], Loss: 0.4351\n",
            "Epoch [6/100], Loss: 0.5418\n",
            "Epoch [6/100], Loss: 0.6440\n",
            "Epoch [6/100], Loss: 0.8526\n",
            "Epoch [6/100], Loss: 1.0141\n",
            "Epoch [6/100], Loss: 1.0856\n",
            "Epoch [7/100], Loss: 0.0603\n",
            "Epoch [7/100], Loss: 0.1650\n",
            "Epoch [7/100], Loss: 0.3169\n",
            "Epoch [7/100], Loss: 0.3779\n",
            "Epoch [7/100], Loss: 0.5894\n",
            "Epoch [7/100], Loss: 0.6212\n",
            "Epoch [7/100], Loss: 0.7084\n",
            "Epoch [7/100], Loss: 0.8265\n",
            "Epoch [8/100], Loss: 0.0526\n",
            "Epoch [8/100], Loss: 0.1138\n",
            "Epoch [8/100], Loss: 0.2615\n",
            "Epoch [8/100], Loss: 0.3729\n",
            "Epoch [8/100], Loss: 0.4088\n",
            "Epoch [8/100], Loss: 0.4780\n",
            "Epoch [8/100], Loss: 0.5785\n",
            "Epoch [8/100], Loss: 0.6341\n",
            "Epoch [9/100], Loss: 0.1158\n",
            "Epoch [9/100], Loss: 0.2866\n",
            "Epoch [9/100], Loss: 0.3724\n",
            "Epoch [9/100], Loss: 0.4873\n",
            "Epoch [9/100], Loss: 0.5088\n",
            "Epoch [9/100], Loss: 0.5190\n",
            "Epoch [9/100], Loss: 0.6065\n",
            "Epoch [9/100], Loss: 0.6101\n",
            "Epoch [10/100], Loss: 0.0658\n",
            "Epoch [10/100], Loss: 0.1405\n",
            "Epoch [10/100], Loss: 0.1933\n",
            "Epoch [10/100], Loss: 0.2396\n",
            "Epoch [10/100], Loss: 0.2666\n",
            "Epoch [10/100], Loss: 0.4670\n",
            "Epoch [10/100], Loss: 0.4946\n",
            "Epoch [10/100], Loss: 0.5348\n",
            "Epoch [11/100], Loss: 0.0682\n",
            "Epoch [11/100], Loss: 0.0980\n",
            "Epoch [11/100], Loss: 0.1833\n",
            "Epoch [11/100], Loss: 0.2680\n",
            "Epoch [11/100], Loss: 0.3762\n",
            "Epoch [11/100], Loss: 0.4236\n",
            "Epoch [11/100], Loss: 0.4475\n",
            "Epoch [11/100], Loss: 0.5125\n",
            "Epoch [12/100], Loss: 0.0583\n",
            "Epoch [12/100], Loss: 0.0788\n",
            "Epoch [12/100], Loss: 0.1595\n",
            "Epoch [12/100], Loss: 0.1708\n",
            "Epoch [12/100], Loss: 0.2170\n",
            "Epoch [12/100], Loss: 0.3557\n",
            "Epoch [12/100], Loss: 0.4380\n",
            "Epoch [12/100], Loss: 0.4556\n",
            "Epoch [13/100], Loss: 0.0202\n",
            "Epoch [13/100], Loss: 0.0979\n",
            "Epoch [13/100], Loss: 0.1572\n",
            "Epoch [13/100], Loss: 0.1651\n",
            "Epoch [13/100], Loss: 0.1712\n",
            "Epoch [13/100], Loss: 0.2059\n",
            "Epoch [13/100], Loss: 0.3394\n",
            "Epoch [13/100], Loss: 0.5056\n",
            "Epoch [14/100], Loss: 0.1617\n",
            "Epoch [14/100], Loss: 0.1808\n",
            "Epoch [14/100], Loss: 0.1999\n",
            "Epoch [14/100], Loss: 0.2038\n",
            "Epoch [14/100], Loss: 0.2365\n",
            "Epoch [14/100], Loss: 0.2494\n",
            "Epoch [14/100], Loss: 0.3813\n",
            "Epoch [14/100], Loss: 0.3871\n",
            "Epoch [15/100], Loss: 0.0767\n",
            "Epoch [15/100], Loss: 0.0878\n",
            "Epoch [15/100], Loss: 0.0965\n",
            "Epoch [15/100], Loss: 0.1003\n",
            "Epoch [15/100], Loss: 0.1503\n",
            "Epoch [15/100], Loss: 0.2735\n",
            "Epoch [15/100], Loss: 0.3254\n",
            "Epoch [15/100], Loss: 0.5724\n",
            "Epoch [16/100], Loss: 0.0212\n",
            "Epoch [16/100], Loss: 0.0248\n",
            "Epoch [16/100], Loss: 0.0884\n",
            "Epoch [16/100], Loss: 0.1307\n",
            "Epoch [16/100], Loss: 0.2360\n",
            "Epoch [16/100], Loss: 0.2486\n",
            "Epoch [16/100], Loss: 0.4381\n",
            "Epoch [16/100], Loss: 0.5509\n",
            "Epoch [17/100], Loss: 0.0952\n",
            "Epoch [17/100], Loss: 0.1243\n",
            "Epoch [17/100], Loss: 0.1268\n",
            "Epoch [17/100], Loss: 0.2245\n",
            "Epoch [17/100], Loss: 0.2834\n",
            "Epoch [17/100], Loss: 0.4482\n",
            "Epoch [17/100], Loss: 0.4575\n",
            "Epoch [17/100], Loss: 0.4589\n",
            "Epoch [18/100], Loss: 0.0435\n",
            "Epoch [18/100], Loss: 0.0646\n",
            "Epoch [18/100], Loss: 0.1689\n",
            "Epoch [18/100], Loss: 0.3399\n",
            "Epoch [18/100], Loss: 0.3424\n",
            "Epoch [18/100], Loss: 0.3555\n",
            "Epoch [18/100], Loss: 0.4002\n",
            "Epoch [18/100], Loss: 0.5400\n",
            "Epoch [19/100], Loss: 0.0113\n",
            "Epoch [19/100], Loss: 0.0169\n",
            "Epoch [19/100], Loss: 0.0587\n",
            "Epoch [19/100], Loss: 0.1573\n",
            "Epoch [19/100], Loss: 0.1749\n",
            "Epoch [19/100], Loss: 0.2059\n",
            "Epoch [19/100], Loss: 0.3410\n",
            "Epoch [19/100], Loss: 0.3855\n",
            "Epoch [20/100], Loss: 0.0781\n",
            "Epoch [20/100], Loss: 0.1086\n",
            "Epoch [20/100], Loss: 0.1136\n",
            "Epoch [20/100], Loss: 0.1588\n",
            "Epoch [20/100], Loss: 0.2988\n",
            "Epoch [20/100], Loss: 0.3807\n",
            "Epoch [20/100], Loss: 0.4264\n",
            "Epoch [20/100], Loss: 0.5413\n",
            "Epoch [21/100], Loss: 0.0031\n",
            "Epoch [21/100], Loss: 0.0191\n",
            "Epoch [21/100], Loss: 0.0244\n",
            "Epoch [21/100], Loss: 0.2032\n",
            "Epoch [21/100], Loss: 0.2713\n",
            "Epoch [21/100], Loss: 0.3887\n",
            "Epoch [21/100], Loss: 0.4529\n",
            "Epoch [21/100], Loss: 0.4670\n",
            "Epoch [22/100], Loss: 0.0336\n",
            "Epoch [22/100], Loss: 0.0785\n",
            "Epoch [22/100], Loss: 0.1159\n",
            "Epoch [22/100], Loss: 0.1326\n",
            "Epoch [22/100], Loss: 0.2494\n",
            "Epoch [22/100], Loss: 0.3699\n",
            "Epoch [22/100], Loss: 0.3728\n",
            "Epoch [22/100], Loss: 0.3945\n",
            "Epoch [23/100], Loss: 0.0764\n",
            "Epoch [23/100], Loss: 0.1088\n",
            "Epoch [23/100], Loss: 0.1108\n",
            "Epoch [23/100], Loss: 0.1446\n",
            "Epoch [23/100], Loss: 0.1655\n",
            "Epoch [23/100], Loss: 0.3877\n",
            "Epoch [23/100], Loss: 0.3967\n",
            "Epoch [23/100], Loss: 0.3995\n",
            "Epoch [24/100], Loss: 0.0715\n",
            "Epoch [24/100], Loss: 0.1025\n",
            "Epoch [24/100], Loss: 0.1045\n",
            "Epoch [24/100], Loss: 0.1137\n",
            "Epoch [24/100], Loss: 0.2364\n",
            "Epoch [24/100], Loss: 0.3134\n",
            "Epoch [24/100], Loss: 0.3388\n",
            "Epoch [24/100], Loss: 0.3773\n",
            "Epoch [25/100], Loss: 0.0261\n",
            "Epoch [25/100], Loss: 0.0468\n",
            "Epoch [25/100], Loss: 0.0499\n",
            "Epoch [25/100], Loss: 0.1618\n",
            "Epoch [25/100], Loss: 0.1848\n",
            "Epoch [25/100], Loss: 0.2187\n",
            "Epoch [25/100], Loss: 0.2298\n",
            "Epoch [25/100], Loss: 0.5180\n",
            "Epoch [26/100], Loss: 0.0030\n",
            "Epoch [26/100], Loss: 0.0424\n",
            "Epoch [26/100], Loss: 0.0466\n",
            "Epoch [26/100], Loss: 0.1703\n",
            "Epoch [26/100], Loss: 0.2867\n",
            "Epoch [26/100], Loss: 0.3030\n",
            "Epoch [26/100], Loss: 0.3350\n",
            "Epoch [26/100], Loss: 0.3368\n",
            "Epoch [27/100], Loss: 0.0036\n",
            "Epoch [27/100], Loss: 0.1023\n",
            "Epoch [27/100], Loss: 0.1453\n",
            "Epoch [27/100], Loss: 0.2685\n",
            "Epoch [27/100], Loss: 0.2927\n",
            "Epoch [27/100], Loss: 0.2948\n",
            "Epoch [27/100], Loss: 0.3369\n",
            "Epoch [27/100], Loss: 0.3415\n",
            "Epoch [28/100], Loss: 0.1787\n",
            "Epoch [28/100], Loss: 0.1989\n",
            "Epoch [28/100], Loss: 0.2019\n",
            "Epoch [28/100], Loss: 0.2054\n",
            "Epoch [28/100], Loss: 0.2785\n",
            "Epoch [28/100], Loss: 0.2809\n",
            "Epoch [28/100], Loss: 0.3135\n",
            "Epoch [28/100], Loss: 0.3592\n",
            "Epoch [29/100], Loss: 0.0012\n",
            "Epoch [29/100], Loss: 0.0239\n",
            "Epoch [29/100], Loss: 0.0351\n",
            "Epoch [29/100], Loss: 0.0761\n",
            "Epoch [29/100], Loss: 0.0795\n",
            "Epoch [29/100], Loss: 0.2174\n",
            "Epoch [29/100], Loss: 0.3237\n",
            "Epoch [29/100], Loss: 0.3696\n",
            "Epoch [30/100], Loss: 0.0027\n",
            "Epoch [30/100], Loss: 0.0270\n",
            "Epoch [30/100], Loss: 0.0490\n",
            "Epoch [30/100], Loss: 0.2900\n",
            "Epoch [30/100], Loss: 0.3050\n",
            "Epoch [30/100], Loss: 0.3068\n",
            "Epoch [30/100], Loss: 0.3269\n",
            "Epoch [30/100], Loss: 0.3285\n",
            "Epoch [31/100], Loss: 0.0148\n",
            "Epoch [31/100], Loss: 0.1261\n",
            "Epoch [31/100], Loss: 0.1303\n",
            "Epoch [31/100], Loss: 0.1360\n",
            "Epoch [31/100], Loss: 0.1380\n",
            "Epoch [31/100], Loss: 0.3347\n",
            "Epoch [31/100], Loss: 0.3405\n",
            "Epoch [31/100], Loss: 0.3770\n",
            "Epoch [32/100], Loss: 0.0939\n",
            "Epoch [32/100], Loss: 0.1166\n",
            "Epoch [32/100], Loss: 0.2265\n",
            "Epoch [32/100], Loss: 0.2568\n",
            "Epoch [32/100], Loss: 0.2599\n",
            "Epoch [32/100], Loss: 0.2879\n",
            "Epoch [32/100], Loss: 0.3230\n",
            "Epoch [32/100], Loss: 0.3257\n",
            "Epoch [33/100], Loss: 0.0257\n",
            "Epoch [33/100], Loss: 0.1271\n",
            "Epoch [33/100], Loss: 0.1730\n",
            "Epoch [33/100], Loss: 0.1758\n",
            "Epoch [33/100], Loss: 0.1918\n",
            "Epoch [33/100], Loss: 0.1932\n",
            "Epoch [33/100], Loss: 0.2050\n",
            "Epoch [33/100], Loss: 0.5363\n",
            "Epoch [34/100], Loss: 0.0054\n",
            "Epoch [34/100], Loss: 0.1122\n",
            "Epoch [34/100], Loss: 0.1145\n",
            "Epoch [34/100], Loss: 0.1539\n",
            "Epoch [34/100], Loss: 0.1633\n",
            "Epoch [34/100], Loss: 0.1858\n",
            "Epoch [34/100], Loss: 0.2179\n",
            "Epoch [34/100], Loss: 0.3823\n",
            "Epoch [35/100], Loss: 0.0019\n",
            "Epoch [35/100], Loss: 0.0746\n",
            "Epoch [35/100], Loss: 0.1046\n",
            "Epoch [35/100], Loss: 0.1182\n",
            "Epoch [35/100], Loss: 0.2147\n",
            "Epoch [35/100], Loss: 0.3834\n",
            "Epoch [35/100], Loss: 0.3850\n",
            "Epoch [35/100], Loss: 0.3864\n",
            "Epoch [36/100], Loss: 0.0010\n",
            "Epoch [36/100], Loss: 0.0074\n",
            "Epoch [36/100], Loss: 0.0121\n",
            "Epoch [36/100], Loss: 0.0179\n",
            "Epoch [36/100], Loss: 0.0385\n",
            "Epoch [36/100], Loss: 0.1676\n",
            "Epoch [36/100], Loss: 0.4139\n",
            "Epoch [36/100], Loss: 0.7542\n",
            "Epoch [37/100], Loss: 0.0026\n",
            "Epoch [37/100], Loss: 0.0193\n",
            "Epoch [37/100], Loss: 0.0935\n",
            "Epoch [37/100], Loss: 0.3052\n",
            "Epoch [37/100], Loss: 0.3965\n",
            "Epoch [37/100], Loss: 0.5111\n",
            "Epoch [37/100], Loss: 0.5751\n",
            "Epoch [37/100], Loss: 0.5971\n",
            "Epoch [38/100], Loss: 0.0039\n",
            "Epoch [38/100], Loss: 0.0441\n",
            "Epoch [38/100], Loss: 0.0514\n",
            "Epoch [38/100], Loss: 0.0545\n",
            "Epoch [38/100], Loss: 0.3329\n",
            "Epoch [38/100], Loss: 0.3384\n",
            "Epoch [38/100], Loss: 0.4016\n",
            "Epoch [38/100], Loss: 0.4106\n",
            "Epoch [39/100], Loss: 0.0133\n",
            "Epoch [39/100], Loss: 0.0172\n",
            "Epoch [39/100], Loss: 0.0263\n",
            "Epoch [39/100], Loss: 0.0373\n",
            "Epoch [39/100], Loss: 0.1303\n",
            "Epoch [39/100], Loss: 0.3191\n",
            "Epoch [39/100], Loss: 0.3311\n",
            "Epoch [39/100], Loss: 0.3331\n",
            "Epoch [40/100], Loss: 0.1159\n",
            "Epoch [40/100], Loss: 0.2464\n",
            "Epoch [40/100], Loss: 0.2896\n",
            "Epoch [40/100], Loss: 0.2969\n",
            "Epoch [40/100], Loss: 0.3100\n",
            "Epoch [40/100], Loss: 0.3187\n",
            "Epoch [40/100], Loss: 0.3279\n",
            "Epoch [40/100], Loss: 0.3300\n",
            "Epoch [41/100], Loss: 0.1215\n",
            "Epoch [41/100], Loss: 0.1324\n",
            "Epoch [41/100], Loss: 0.1347\n",
            "Epoch [41/100], Loss: 0.1584\n",
            "Epoch [41/100], Loss: 0.1758\n",
            "Epoch [41/100], Loss: 0.1778\n",
            "Epoch [41/100], Loss: 0.3084\n",
            "Epoch [41/100], Loss: 0.3704\n",
            "Epoch [42/100], Loss: 0.0054\n",
            "Epoch [42/100], Loss: 0.1166\n",
            "Epoch [42/100], Loss: 0.1489\n",
            "Epoch [42/100], Loss: 0.2963\n",
            "Epoch [42/100], Loss: 0.2992\n",
            "Epoch [42/100], Loss: 0.3006\n",
            "Epoch [42/100], Loss: 0.3149\n",
            "Epoch [42/100], Loss: 0.3374\n",
            "Epoch [43/100], Loss: 0.0447\n",
            "Epoch [43/100], Loss: 0.0478\n",
            "Epoch [43/100], Loss: 0.1705\n",
            "Epoch [43/100], Loss: 0.2026\n",
            "Epoch [43/100], Loss: 0.2039\n",
            "Epoch [43/100], Loss: 0.3197\n",
            "Epoch [43/100], Loss: 0.3216\n",
            "Epoch [43/100], Loss: 0.3379\n",
            "Epoch [44/100], Loss: 0.0270\n",
            "Epoch [44/100], Loss: 0.0409\n",
            "Epoch [44/100], Loss: 0.0502\n",
            "Epoch [44/100], Loss: 0.0594\n",
            "Epoch [44/100], Loss: 0.1661\n",
            "Epoch [44/100], Loss: 0.2892\n",
            "Epoch [44/100], Loss: 0.2903\n",
            "Epoch [44/100], Loss: 0.2999\n",
            "Epoch [45/100], Loss: 0.0007\n",
            "Epoch [45/100], Loss: 0.0223\n",
            "Epoch [45/100], Loss: 0.0272\n",
            "Epoch [45/100], Loss: 0.0412\n",
            "Epoch [45/100], Loss: 0.1534\n",
            "Epoch [45/100], Loss: 0.1555\n",
            "Epoch [45/100], Loss: 0.2936\n",
            "Epoch [45/100], Loss: 0.3030\n",
            "Epoch [46/100], Loss: 0.1639\n",
            "Epoch [46/100], Loss: 0.1724\n",
            "Epoch [46/100], Loss: 0.2122\n",
            "Epoch [46/100], Loss: 0.3631\n",
            "Epoch [46/100], Loss: 0.3646\n",
            "Epoch [46/100], Loss: 0.4414\n",
            "Epoch [46/100], Loss: 0.4545\n",
            "Epoch [46/100], Loss: 0.4555\n",
            "Epoch [47/100], Loss: 0.0066\n",
            "Epoch [47/100], Loss: 0.0215\n",
            "Epoch [47/100], Loss: 0.0243\n",
            "Epoch [47/100], Loss: 0.1682\n",
            "Epoch [47/100], Loss: 0.2370\n",
            "Epoch [47/100], Loss: 0.2420\n",
            "Epoch [47/100], Loss: 0.3346\n",
            "Epoch [47/100], Loss: 0.3363\n",
            "Epoch [48/100], Loss: 0.0068\n",
            "Epoch [48/100], Loss: 0.0174\n",
            "Epoch [48/100], Loss: 0.0310\n",
            "Epoch [48/100], Loss: 0.2060\n",
            "Epoch [48/100], Loss: 0.2248\n",
            "Epoch [48/100], Loss: 0.3343\n",
            "Epoch [48/100], Loss: 0.3378\n",
            "Epoch [48/100], Loss: 0.3387\n",
            "Epoch [49/100], Loss: 0.0860\n",
            "Epoch [49/100], Loss: 0.0878\n",
            "Epoch [49/100], Loss: 0.1019\n",
            "Epoch [49/100], Loss: 0.1693\n",
            "Epoch [49/100], Loss: 0.1808\n",
            "Epoch [49/100], Loss: 0.1884\n",
            "Epoch [49/100], Loss: 0.2094\n",
            "Epoch [49/100], Loss: 0.3874\n",
            "Epoch [50/100], Loss: 0.0505\n",
            "Epoch [50/100], Loss: 0.1002\n",
            "Epoch [50/100], Loss: 0.1044\n",
            "Epoch [50/100], Loss: 0.1127\n",
            "Epoch [50/100], Loss: 0.4878\n",
            "Epoch [50/100], Loss: 0.5118\n",
            "Epoch [50/100], Loss: 0.5132\n",
            "Epoch [50/100], Loss: 0.5136\n",
            "Epoch [51/100], Loss: 0.0029\n",
            "Epoch [51/100], Loss: 0.0177\n",
            "Epoch [51/100], Loss: 0.0443\n",
            "Epoch [51/100], Loss: 0.0494\n",
            "Epoch [51/100], Loss: 0.1654\n",
            "Epoch [51/100], Loss: 0.2723\n",
            "Epoch [51/100], Loss: 0.2919\n",
            "Epoch [51/100], Loss: 0.3338\n",
            "Epoch [52/100], Loss: 0.0810\n",
            "Epoch [52/100], Loss: 0.1208\n",
            "Epoch [52/100], Loss: 0.1725\n",
            "Epoch [52/100], Loss: 0.1754\n",
            "Epoch [52/100], Loss: 0.3532\n",
            "Epoch [52/100], Loss: 0.4166\n",
            "Epoch [52/100], Loss: 0.4196\n",
            "Epoch [52/100], Loss: 0.4355\n",
            "Epoch [53/100], Loss: 0.0142\n",
            "Epoch [53/100], Loss: 0.1615\n",
            "Epoch [53/100], Loss: 0.1646\n",
            "Epoch [53/100], Loss: 0.1985\n",
            "Epoch [53/100], Loss: 0.2901\n",
            "Epoch [53/100], Loss: 0.3123\n",
            "Epoch [53/100], Loss: 0.3141\n",
            "Epoch [53/100], Loss: 0.3179\n",
            "Epoch [54/100], Loss: 0.0036\n",
            "Epoch [54/100], Loss: 0.1781\n",
            "Epoch [54/100], Loss: 0.2322\n",
            "Epoch [54/100], Loss: 0.2967\n",
            "Epoch [54/100], Loss: 0.3011\n",
            "Epoch [54/100], Loss: 0.3076\n",
            "Epoch [54/100], Loss: 0.3182\n",
            "Epoch [54/100], Loss: 0.3573\n",
            "Epoch [55/100], Loss: 0.0700\n",
            "Epoch [55/100], Loss: 0.1287\n",
            "Epoch [55/100], Loss: 0.1414\n",
            "Epoch [55/100], Loss: 0.1538\n",
            "Epoch [55/100], Loss: 0.2705\n",
            "Epoch [55/100], Loss: 0.2711\n",
            "Epoch [55/100], Loss: 0.2835\n",
            "Epoch [55/100], Loss: 0.2844\n",
            "Epoch [56/100], Loss: 0.0010\n",
            "Epoch [56/100], Loss: 0.0371\n",
            "Epoch [56/100], Loss: 0.0408\n",
            "Epoch [56/100], Loss: 0.2383\n",
            "Epoch [56/100], Loss: 0.2431\n",
            "Epoch [56/100], Loss: 0.3449\n",
            "Epoch [56/100], Loss: 0.3590\n",
            "Epoch [56/100], Loss: 0.4316\n",
            "Epoch [57/100], Loss: 0.0090\n",
            "Epoch [57/100], Loss: 0.1208\n",
            "Epoch [57/100], Loss: 0.2040\n",
            "Epoch [57/100], Loss: 0.2453\n",
            "Epoch [57/100], Loss: 0.2685\n",
            "Epoch [57/100], Loss: 0.2708\n",
            "Epoch [57/100], Loss: 0.2749\n",
            "Epoch [57/100], Loss: 0.2771\n",
            "Epoch [58/100], Loss: 0.2031\n",
            "Epoch [58/100], Loss: 0.2773\n",
            "Epoch [58/100], Loss: 0.2972\n",
            "Epoch [58/100], Loss: 0.2996\n",
            "Epoch [58/100], Loss: 0.3069\n",
            "Epoch [58/100], Loss: 0.3084\n",
            "Epoch [58/100], Loss: 0.3298\n",
            "Epoch [58/100], Loss: 0.3491\n",
            "Epoch [59/100], Loss: 0.0008\n",
            "Epoch [59/100], Loss: 0.0064\n",
            "Epoch [59/100], Loss: 0.0160\n",
            "Epoch [59/100], Loss: 0.2680\n",
            "Epoch [59/100], Loss: 0.2773\n",
            "Epoch [59/100], Loss: 0.2832\n",
            "Epoch [59/100], Loss: 0.2857\n",
            "Epoch [59/100], Loss: 0.2874\n",
            "Epoch [60/100], Loss: 0.0257\n",
            "Epoch [60/100], Loss: 0.0559\n",
            "Epoch [60/100], Loss: 0.1078\n",
            "Epoch [60/100], Loss: 0.1346\n",
            "Epoch [60/100], Loss: 0.1699\n",
            "Epoch [60/100], Loss: 0.1975\n",
            "Epoch [60/100], Loss: 0.3077\n",
            "Epoch [60/100], Loss: 0.3082\n",
            "Epoch [61/100], Loss: 0.0772\n",
            "Epoch [61/100], Loss: 0.0972\n",
            "Epoch [61/100], Loss: 0.0979\n",
            "Epoch [61/100], Loss: 0.1000\n",
            "Epoch [61/100], Loss: 0.4747\n",
            "Epoch [61/100], Loss: 0.4755\n",
            "Epoch [61/100], Loss: 0.4779\n",
            "Epoch [61/100], Loss: 0.4784\n",
            "Epoch [62/100], Loss: 0.0668\n",
            "Epoch [62/100], Loss: 0.0770\n",
            "Epoch [62/100], Loss: 0.0779\n",
            "Epoch [62/100], Loss: 0.0823\n",
            "Epoch [62/100], Loss: 0.0852\n",
            "Epoch [62/100], Loss: 0.0965\n",
            "Epoch [62/100], Loss: 0.1158\n",
            "Epoch [62/100], Loss: 0.3102\n",
            "Epoch [63/100], Loss: 0.0269\n",
            "Epoch [63/100], Loss: 0.0544\n",
            "Epoch [63/100], Loss: 0.1328\n",
            "Epoch [63/100], Loss: 0.1456\n",
            "Epoch [63/100], Loss: 0.3548\n",
            "Epoch [63/100], Loss: 0.3568\n",
            "Epoch [63/100], Loss: 0.3809\n",
            "Epoch [63/100], Loss: 0.3861\n",
            "Epoch [64/100], Loss: 0.0009\n",
            "Epoch [64/100], Loss: 0.0283\n",
            "Epoch [64/100], Loss: 0.0949\n",
            "Epoch [64/100], Loss: 0.0986\n",
            "Epoch [64/100], Loss: 0.2657\n",
            "Epoch [64/100], Loss: 0.2685\n",
            "Epoch [64/100], Loss: 0.3188\n",
            "Epoch [64/100], Loss: 0.5394\n",
            "Epoch [65/100], Loss: 0.0247\n",
            "Epoch [65/100], Loss: 0.0263\n",
            "Epoch [65/100], Loss: 0.0691\n",
            "Epoch [65/100], Loss: 0.0831\n",
            "Epoch [65/100], Loss: 0.1012\n",
            "Epoch [65/100], Loss: 0.1309\n",
            "Epoch [65/100], Loss: 0.3499\n",
            "Epoch [65/100], Loss: 0.3514\n",
            "Epoch [66/100], Loss: 0.0250\n",
            "Epoch [66/100], Loss: 0.0335\n",
            "Epoch [66/100], Loss: 0.0455\n",
            "Epoch [66/100], Loss: 0.0656\n",
            "Epoch [66/100], Loss: 0.0862\n",
            "Epoch [66/100], Loss: 0.1490\n",
            "Epoch [66/100], Loss: 0.3091\n",
            "Epoch [66/100], Loss: 0.3101\n",
            "Epoch [67/100], Loss: 0.0010\n",
            "Epoch [67/100], Loss: 0.0031\n",
            "Epoch [67/100], Loss: 0.0198\n",
            "Epoch [67/100], Loss: 0.0285\n",
            "Epoch [67/100], Loss: 0.0383\n",
            "Epoch [67/100], Loss: 0.0521\n",
            "Epoch [67/100], Loss: 0.2550\n",
            "Epoch [67/100], Loss: 0.2562\n",
            "Epoch [68/100], Loss: 0.0139\n",
            "Epoch [68/100], Loss: 0.0200\n",
            "Epoch [68/100], Loss: 0.0404\n",
            "Epoch [68/100], Loss: 0.0761\n",
            "Epoch [68/100], Loss: 0.1399\n",
            "Epoch [68/100], Loss: 0.1439\n",
            "Epoch [68/100], Loss: 0.2982\n",
            "Epoch [68/100], Loss: 0.2986\n",
            "Epoch [69/100], Loss: 0.0016\n",
            "Epoch [69/100], Loss: 0.0096\n",
            "Epoch [69/100], Loss: 0.0691\n",
            "Epoch [69/100], Loss: 0.0715\n",
            "Epoch [69/100], Loss: 0.0724\n",
            "Epoch [69/100], Loss: 0.2551\n",
            "Epoch [69/100], Loss: 0.2688\n",
            "Epoch [69/100], Loss: 0.5582\n",
            "Epoch [70/100], Loss: 0.0364\n",
            "Epoch [70/100], Loss: 0.0517\n",
            "Epoch [70/100], Loss: 0.0542\n",
            "Epoch [70/100], Loss: 0.0646\n",
            "Epoch [70/100], Loss: 0.1351\n",
            "Epoch [70/100], Loss: 0.1905\n",
            "Epoch [70/100], Loss: 0.2300\n",
            "Epoch [70/100], Loss: 0.3215\n",
            "Epoch [71/100], Loss: 0.0110\n",
            "Epoch [71/100], Loss: 0.0267\n",
            "Epoch [71/100], Loss: 0.0385\n",
            "Epoch [71/100], Loss: 0.0492\n",
            "Epoch [71/100], Loss: 0.1968\n",
            "Epoch [71/100], Loss: 0.2597\n",
            "Epoch [71/100], Loss: 0.2641\n",
            "Epoch [71/100], Loss: 0.2731\n",
            "Epoch [72/100], Loss: 0.0042\n",
            "Epoch [72/100], Loss: 0.0545\n",
            "Epoch [72/100], Loss: 0.1128\n",
            "Epoch [72/100], Loss: 0.1433\n",
            "Epoch [72/100], Loss: 0.1476\n",
            "Epoch [72/100], Loss: 0.1551\n",
            "Epoch [72/100], Loss: 0.2993\n",
            "Epoch [72/100], Loss: 0.3055\n",
            "Epoch [73/100], Loss: 0.0006\n",
            "Epoch [73/100], Loss: 0.0140\n",
            "Epoch [73/100], Loss: 0.0452\n",
            "Epoch [73/100], Loss: 0.1171\n",
            "Epoch [73/100], Loss: 0.1412\n",
            "Epoch [73/100], Loss: 0.1431\n",
            "Epoch [73/100], Loss: 0.2724\n",
            "Epoch [73/100], Loss: 0.2794\n",
            "Epoch [74/100], Loss: 0.0019\n",
            "Epoch [74/100], Loss: 0.0025\n",
            "Epoch [74/100], Loss: 0.0210\n",
            "Epoch [74/100], Loss: 0.1608\n",
            "Epoch [74/100], Loss: 0.1961\n",
            "Epoch [74/100], Loss: 0.2231\n",
            "Epoch [74/100], Loss: 0.2243\n",
            "Epoch [74/100], Loss: 0.2304\n",
            "Epoch [75/100], Loss: 0.0434\n",
            "Epoch [75/100], Loss: 0.0449\n",
            "Epoch [75/100], Loss: 0.1461\n",
            "Epoch [75/100], Loss: 0.1471\n",
            "Epoch [75/100], Loss: 0.1816\n",
            "Epoch [75/100], Loss: 0.1986\n",
            "Epoch [75/100], Loss: 0.2779\n",
            "Epoch [75/100], Loss: 0.2861\n",
            "Epoch [76/100], Loss: 0.0010\n",
            "Epoch [76/100], Loss: 0.0702\n",
            "Epoch [76/100], Loss: 0.0773\n",
            "Epoch [76/100], Loss: 0.0780\n",
            "Epoch [76/100], Loss: 0.0827\n",
            "Epoch [76/100], Loss: 0.3009\n",
            "Epoch [76/100], Loss: 0.3053\n",
            "Epoch [76/100], Loss: 0.3065\n",
            "Epoch [77/100], Loss: 0.0010\n",
            "Epoch [77/100], Loss: 0.0592\n",
            "Epoch [77/100], Loss: 0.0607\n",
            "Epoch [77/100], Loss: 0.1408\n",
            "Epoch [77/100], Loss: 0.1515\n",
            "Epoch [77/100], Loss: 0.1712\n",
            "Epoch [77/100], Loss: 0.2078\n",
            "Epoch [77/100], Loss: 0.2088\n",
            "Epoch [78/100], Loss: 0.0005\n",
            "Epoch [78/100], Loss: 0.0331\n",
            "Epoch [78/100], Loss: 0.0929\n",
            "Epoch [78/100], Loss: 0.0953\n",
            "Epoch [78/100], Loss: 0.1964\n",
            "Epoch [78/100], Loss: 0.2152\n",
            "Epoch [78/100], Loss: 0.2265\n",
            "Epoch [78/100], Loss: 0.2274\n",
            "Epoch [79/100], Loss: 0.0863\n",
            "Epoch [79/100], Loss: 0.0871\n",
            "Epoch [79/100], Loss: 0.0907\n",
            "Epoch [79/100], Loss: 0.1208\n",
            "Epoch [79/100], Loss: 0.1239\n",
            "Epoch [79/100], Loss: 0.1355\n",
            "Epoch [79/100], Loss: 0.2217\n",
            "Epoch [79/100], Loss: 0.2294\n",
            "Epoch [80/100], Loss: 0.0359\n",
            "Epoch [80/100], Loss: 0.0399\n",
            "Epoch [80/100], Loss: 0.1609\n",
            "Epoch [80/100], Loss: 0.1659\n",
            "Epoch [80/100], Loss: 0.1820\n",
            "Epoch [80/100], Loss: 0.1825\n",
            "Epoch [80/100], Loss: 0.1899\n",
            "Epoch [80/100], Loss: 0.1933\n",
            "Epoch [81/100], Loss: 0.0264\n",
            "Epoch [81/100], Loss: 0.0335\n",
            "Epoch [81/100], Loss: 0.0928\n",
            "Epoch [81/100], Loss: 0.1999\n",
            "Epoch [81/100], Loss: 0.2014\n",
            "Epoch [81/100], Loss: 0.2135\n",
            "Epoch [81/100], Loss: 0.2176\n",
            "Epoch [81/100], Loss: 0.2208\n",
            "Epoch [82/100], Loss: 0.0182\n",
            "Epoch [82/100], Loss: 0.0428\n",
            "Epoch [82/100], Loss: 0.0444\n",
            "Epoch [82/100], Loss: 0.0836\n",
            "Epoch [82/100], Loss: 0.0844\n",
            "Epoch [82/100], Loss: 0.0942\n",
            "Epoch [82/100], Loss: 0.2028\n",
            "Epoch [82/100], Loss: 0.2042\n",
            "Epoch [83/100], Loss: 0.0037\n",
            "Epoch [83/100], Loss: 0.0085\n",
            "Epoch [83/100], Loss: 0.0097\n",
            "Epoch [83/100], Loss: 0.0104\n",
            "Epoch [83/100], Loss: 0.0540\n",
            "Epoch [83/100], Loss: 0.0936\n",
            "Epoch [83/100], Loss: 0.1724\n",
            "Epoch [83/100], Loss: 0.1950\n",
            "Epoch [84/100], Loss: 0.0059\n",
            "Epoch [84/100], Loss: 0.0438\n",
            "Epoch [84/100], Loss: 0.1527\n",
            "Epoch [84/100], Loss: 0.1965\n",
            "Epoch [84/100], Loss: 0.2196\n",
            "Epoch [84/100], Loss: 0.2350\n",
            "Epoch [84/100], Loss: 0.2379\n",
            "Epoch [84/100], Loss: 0.2385\n",
            "Epoch [85/100], Loss: 0.0060\n",
            "Epoch [85/100], Loss: 0.0321\n",
            "Epoch [85/100], Loss: 0.0439\n",
            "Epoch [85/100], Loss: 0.0500\n",
            "Epoch [85/100], Loss: 0.0504\n",
            "Epoch [85/100], Loss: 0.0865\n",
            "Epoch [85/100], Loss: 0.1890\n",
            "Epoch [85/100], Loss: 0.1895\n",
            "Epoch [86/100], Loss: 0.0153\n",
            "Epoch [86/100], Loss: 0.0382\n",
            "Epoch [86/100], Loss: 0.0477\n",
            "Epoch [86/100], Loss: 0.1026\n",
            "Epoch [86/100], Loss: 0.1042\n",
            "Epoch [86/100], Loss: 0.1138\n",
            "Epoch [86/100], Loss: 0.1767\n",
            "Epoch [86/100], Loss: 0.1771\n",
            "Epoch [87/100], Loss: 0.0024\n",
            "Epoch [87/100], Loss: 0.1073\n",
            "Epoch [87/100], Loss: 0.1198\n",
            "Epoch [87/100], Loss: 0.1200\n",
            "Epoch [87/100], Loss: 0.1236\n",
            "Epoch [87/100], Loss: 0.1297\n",
            "Epoch [87/100], Loss: 0.1626\n",
            "Epoch [87/100], Loss: 0.1641\n",
            "Epoch [88/100], Loss: 0.0296\n",
            "Epoch [88/100], Loss: 0.0367\n",
            "Epoch [88/100], Loss: 0.1054\n",
            "Epoch [88/100], Loss: 0.1117\n",
            "Epoch [88/100], Loss: 0.1655\n",
            "Epoch [88/100], Loss: 0.1662\n",
            "Epoch [88/100], Loss: 0.1702\n",
            "Epoch [88/100], Loss: 0.1754\n",
            "Epoch [89/100], Loss: 0.0055\n",
            "Epoch [89/100], Loss: 0.0601\n",
            "Epoch [89/100], Loss: 0.1205\n",
            "Epoch [89/100], Loss: 0.1217\n",
            "Epoch [89/100], Loss: 0.1288\n",
            "Epoch [89/100], Loss: 0.1318\n",
            "Epoch [89/100], Loss: 0.1400\n",
            "Epoch [89/100], Loss: 0.1629\n",
            "Epoch [90/100], Loss: 0.0389\n",
            "Epoch [90/100], Loss: 0.0497\n",
            "Epoch [90/100], Loss: 0.0696\n",
            "Epoch [90/100], Loss: 0.1264\n",
            "Epoch [90/100], Loss: 0.1265\n",
            "Epoch [90/100], Loss: 0.1405\n",
            "Epoch [90/100], Loss: 0.1410\n",
            "Epoch [90/100], Loss: 0.1435\n",
            "Epoch [91/100], Loss: 0.0008\n",
            "Epoch [91/100], Loss: 0.0013\n",
            "Epoch [91/100], Loss: 0.0067\n",
            "Epoch [91/100], Loss: 0.0086\n",
            "Epoch [91/100], Loss: 0.0110\n",
            "Epoch [91/100], Loss: 0.2123\n",
            "Epoch [91/100], Loss: 0.2126\n",
            "Epoch [91/100], Loss: 0.2300\n",
            "Epoch [92/100], Loss: 0.0073\n",
            "Epoch [92/100], Loss: 0.0313\n",
            "Epoch [92/100], Loss: 0.0372\n",
            "Epoch [92/100], Loss: 0.0386\n",
            "Epoch [92/100], Loss: 0.1567\n",
            "Epoch [92/100], Loss: 0.1872\n",
            "Epoch [92/100], Loss: 0.1876\n",
            "Epoch [92/100], Loss: 0.1963\n",
            "Epoch [93/100], Loss: 0.0088\n",
            "Epoch [93/100], Loss: 0.0168\n",
            "Epoch [93/100], Loss: 0.0648\n",
            "Epoch [93/100], Loss: 0.0884\n",
            "Epoch [93/100], Loss: 0.1581\n",
            "Epoch [93/100], Loss: 0.1600\n",
            "Epoch [93/100], Loss: 0.1625\n",
            "Epoch [93/100], Loss: 0.1636\n",
            "Epoch [94/100], Loss: 0.0003\n",
            "Epoch [94/100], Loss: 0.0130\n",
            "Epoch [94/100], Loss: 0.0178\n",
            "Epoch [94/100], Loss: 0.0401\n",
            "Epoch [94/100], Loss: 0.0408\n",
            "Epoch [94/100], Loss: 0.1140\n",
            "Epoch [94/100], Loss: 0.1190\n",
            "Epoch [94/100], Loss: 0.1212\n",
            "Epoch [95/100], Loss: 0.0325\n",
            "Epoch [95/100], Loss: 0.0797\n",
            "Epoch [95/100], Loss: 0.0936\n",
            "Epoch [95/100], Loss: 0.0947\n",
            "Epoch [95/100], Loss: 0.1099\n",
            "Epoch [95/100], Loss: 0.1170\n",
            "Epoch [95/100], Loss: 0.1189\n",
            "Epoch [95/100], Loss: 0.1240\n",
            "Epoch [96/100], Loss: 0.0008\n",
            "Epoch [96/100], Loss: 0.0043\n",
            "Epoch [96/100], Loss: 0.0054\n",
            "Epoch [96/100], Loss: 0.0170\n",
            "Epoch [96/100], Loss: 0.0346\n",
            "Epoch [96/100], Loss: 0.0455\n",
            "Epoch [96/100], Loss: 0.1148\n",
            "Epoch [96/100], Loss: 0.1149\n",
            "Epoch [97/100], Loss: 0.0017\n",
            "Epoch [97/100], Loss: 0.0142\n",
            "Epoch [97/100], Loss: 0.0485\n",
            "Epoch [97/100], Loss: 0.0584\n",
            "Epoch [97/100], Loss: 0.1071\n",
            "Epoch [97/100], Loss: 0.1125\n",
            "Epoch [97/100], Loss: 0.1167\n",
            "Epoch [97/100], Loss: 0.1168\n",
            "Epoch [98/100], Loss: 0.0043\n",
            "Epoch [98/100], Loss: 0.0051\n",
            "Epoch [98/100], Loss: 0.0063\n",
            "Epoch [98/100], Loss: 0.0095\n",
            "Epoch [98/100], Loss: 0.0133\n",
            "Epoch [98/100], Loss: 0.1105\n",
            "Epoch [98/100], Loss: 0.1329\n",
            "Epoch [98/100], Loss: 0.1331\n",
            "Epoch [99/100], Loss: 0.0135\n",
            "Epoch [99/100], Loss: 0.0210\n",
            "Epoch [99/100], Loss: 0.0260\n",
            "Epoch [99/100], Loss: 0.0423\n",
            "Epoch [99/100], Loss: 0.1059\n",
            "Epoch [99/100], Loss: 0.1187\n",
            "Epoch [99/100], Loss: 0.1676\n",
            "Epoch [99/100], Loss: 0.2444\n",
            "Epoch [100/100], Loss: 0.0109\n",
            "Epoch [100/100], Loss: 0.0287\n",
            "Epoch [100/100], Loss: 0.0399\n",
            "Epoch [100/100], Loss: 0.1037\n",
            "Epoch [100/100], Loss: 0.1057\n",
            "Epoch [100/100], Loss: 0.1150\n",
            "Epoch [100/100], Loss: 0.1213\n",
            "Epoch [100/100], Loss: 0.1215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() #Set model to evaluation mode\n",
        "\n",
        "correct = 0 #Number of correct predictions\n",
        "\n",
        "total = 0 #Total samples\n",
        "\n",
        "with torch.no_grad(): #Not tracking gradients this time\n",
        "\n",
        "  for features, labels in test_loader:\n",
        "    # Move tensors to the configured device\n",
        "    features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "    outputs = model(features) #Forward pass\n",
        "    _, predicted = torch.max(outputs,1) #Get index of max logit (predicted class)\n",
        "    correct += (predicted == labels).sum().item() #Count correct items\n",
        "    total += labels.size(0) #Count samples\n",
        "\n",
        "\n",
        "accuracy = correct/total\n",
        "print(f\"Test accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93z4ZiVIWVVr",
        "outputId": "1bf7187b-3159-4c63-d717-85cd4e3d4865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EhBvRXtVKYTR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}