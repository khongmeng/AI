{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ThR-r5YGqiE6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeUglqofquz3"
   },
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ChtpQoi_qseL"
   },
   "outputs": [],
   "source": [
    "#Dimension of the noise vector\n",
    "latent_dim = 100\n",
    "\n",
    "#28x28 images for FashionMNIST\n",
    "image_size = 28 * 28\n",
    "\n",
    "batch_size_gan = 64\n",
    "\n",
    "#Number of training epochs for GAN\n",
    "epochs_gan = 50\n",
    "\n",
    "#Learning rate for both generator and discriminator\n",
    "lr_gan = 0.0002\n",
    "\n",
    "#Adam: decay of first order momentum of all gradients\n",
    "b1 = 0.5\n",
    "\n",
    "#Adam: decay of second order momentum of all gradients\n",
    "b2 = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bd6yffk3q_TS"
   },
   "source": [
    "Load FashionMNIST dataset and define generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyftmojsrCQY",
    "outputId": "244afa45-a21d-477d-f7d2-1fd0c9ae297a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting FashionMNIST GAN Training ---\n",
      "Epoch [1/50] Batch 0/938 D Loss: 0.6879 G Loss: 0.6873\n",
      "Epoch [1/50] Batch 100/938 D Loss: 0.0928 G Loss: 2.5894\n",
      "Epoch [1/50] Batch 200/938 D Loss: 0.3228 G Loss: 1.3730\n",
      "Epoch [1/50] Batch 300/938 D Loss: 0.0880 G Loss: 3.8524\n",
      "Epoch [1/50] Batch 400/938 D Loss: 0.1933 G Loss: 3.1075\n",
      "Epoch [1/50] Batch 500/938 D Loss: 0.3563 G Loss: 3.1617\n",
      "Epoch [1/50] Batch 600/938 D Loss: 0.3961 G Loss: 2.4188\n",
      "Epoch [1/50] Batch 700/938 D Loss: 0.3135 G Loss: 1.9781\n",
      "Epoch [1/50] Batch 800/938 D Loss: 0.4159 G Loss: 3.6421\n",
      "Epoch [1/50] Batch 900/938 D Loss: 0.2984 G Loss: 2.0337\n",
      "Epoch [1/50] Final D Loss: 0.3241 Final G Loss: 3.1112\n",
      "Epoch [2/50] Batch 0/938 D Loss: 0.3419 G Loss: 2.2436\n",
      "Epoch [2/50] Batch 100/938 D Loss: 0.3650 G Loss: 1.5271\n",
      "Epoch [2/50] Batch 200/938 D Loss: 0.4592 G Loss: 2.0512\n",
      "Epoch [2/50] Batch 300/938 D Loss: 0.3271 G Loss: 1.9974\n",
      "Epoch [2/50] Batch 400/938 D Loss: 0.4153 G Loss: 2.5219\n",
      "Epoch [2/50] Batch 500/938 D Loss: 0.4321 G Loss: 2.7222\n",
      "Epoch [2/50] Batch 600/938 D Loss: 0.3987 G Loss: 1.1017\n",
      "Epoch [2/50] Batch 700/938 D Loss: 0.3987 G Loss: 1.8209\n",
      "Epoch [2/50] Batch 800/938 D Loss: 0.4914 G Loss: 2.3182\n",
      "Epoch [2/50] Batch 900/938 D Loss: 0.5311 G Loss: 1.3278\n",
      "Epoch [2/50] Final D Loss: 0.3505 Final G Loss: 2.5611\n",
      "Epoch [3/50] Batch 0/938 D Loss: 0.5119 G Loss: 1.5791\n",
      "Epoch [3/50] Batch 100/938 D Loss: 0.4346 G Loss: 1.6417\n",
      "Epoch [3/50] Batch 200/938 D Loss: 0.4502 G Loss: 1.6403\n",
      "Epoch [3/50] Batch 300/938 D Loss: 0.4497 G Loss: 1.4355\n",
      "Epoch [3/50] Batch 400/938 D Loss: 0.4911 G Loss: 1.7301\n",
      "Epoch [3/50] Batch 500/938 D Loss: 0.4248 G Loss: 1.6272\n",
      "Epoch [3/50] Batch 600/938 D Loss: 0.5213 G Loss: 1.2292\n",
      "Epoch [3/50] Batch 700/938 D Loss: 0.5832 G Loss: 1.2214\n",
      "Epoch [3/50] Batch 800/938 D Loss: 0.4735 G Loss: 1.2782\n",
      "Epoch [3/50] Batch 900/938 D Loss: 0.4368 G Loss: 1.7388\n",
      "Epoch [3/50] Final D Loss: 0.3742 Final G Loss: 2.1834\n",
      "Epoch [4/50] Batch 0/938 D Loss: 0.5433 G Loss: 1.2454\n",
      "Epoch [4/50] Batch 100/938 D Loss: 0.4665 G Loss: 1.6369\n",
      "Epoch [4/50] Batch 200/938 D Loss: 0.4755 G Loss: 1.3507\n",
      "Epoch [4/50] Batch 300/938 D Loss: 0.4108 G Loss: 1.5467\n",
      "Epoch [4/50] Batch 400/938 D Loss: 0.4121 G Loss: 1.8026\n",
      "Epoch [4/50] Batch 500/938 D Loss: 0.4451 G Loss: 1.6324\n",
      "Epoch [4/50] Batch 600/938 D Loss: 0.5690 G Loss: 1.2209\n",
      "Epoch [4/50] Batch 700/938 D Loss: 0.5551 G Loss: 1.0239\n",
      "Epoch [4/50] Batch 800/938 D Loss: 0.5499 G Loss: 1.3000\n",
      "Epoch [4/50] Batch 900/938 D Loss: 0.4952 G Loss: 1.2462\n",
      "Epoch [4/50] Final D Loss: 0.5890 Final G Loss: 1.3951\n",
      "Epoch [5/50] Batch 0/938 D Loss: 0.4720 G Loss: 1.2366\n",
      "Epoch [5/50] Batch 100/938 D Loss: 0.5519 G Loss: 1.1462\n",
      "Epoch [5/50] Batch 200/938 D Loss: 0.5755 G Loss: 1.5803\n",
      "Epoch [5/50] Batch 300/938 D Loss: 0.5114 G Loss: 1.3824\n",
      "Epoch [5/50] Batch 400/938 D Loss: 0.5221 G Loss: 1.3609\n",
      "Epoch [5/50] Batch 500/938 D Loss: 0.5581 G Loss: 1.3234\n",
      "Epoch [5/50] Batch 600/938 D Loss: 0.5977 G Loss: 1.3936\n",
      "Epoch [5/50] Batch 700/938 D Loss: 0.6432 G Loss: 0.9774\n",
      "Epoch [5/50] Batch 800/938 D Loss: 0.5187 G Loss: 1.2638\n",
      "Epoch [5/50] Batch 900/938 D Loss: 0.4963 G Loss: 1.1964\n",
      "Epoch [5/50] Final D Loss: 0.5027 Final G Loss: 1.3395\n",
      "Epoch [6/50] Batch 0/938 D Loss: 0.5610 G Loss: 0.9896\n",
      "Epoch [6/50] Batch 100/938 D Loss: 0.4932 G Loss: 1.2212\n",
      "Epoch [6/50] Batch 200/938 D Loss: 0.6227 G Loss: 0.9122\n",
      "Epoch [6/50] Batch 300/938 D Loss: 0.6143 G Loss: 1.2312\n",
      "Epoch [6/50] Batch 400/938 D Loss: 0.6073 G Loss: 1.1825\n",
      "Epoch [6/50] Batch 500/938 D Loss: 0.6335 G Loss: 1.0524\n",
      "Epoch [6/50] Batch 600/938 D Loss: 0.6162 G Loss: 1.1539\n",
      "Epoch [6/50] Batch 700/938 D Loss: 0.5471 G Loss: 1.2253\n",
      "Epoch [6/50] Batch 800/938 D Loss: 0.5651 G Loss: 1.1702\n",
      "Epoch [6/50] Batch 900/938 D Loss: 0.5785 G Loss: 1.1949\n",
      "Epoch [6/50] Final D Loss: 0.5529 Final G Loss: 1.3932\n",
      "Epoch [7/50] Batch 0/938 D Loss: 0.5686 G Loss: 1.0903\n",
      "Epoch [7/50] Batch 100/938 D Loss: 0.5559 G Loss: 1.1368\n",
      "Epoch [7/50] Batch 200/938 D Loss: 0.5757 G Loss: 0.9987\n",
      "Epoch [7/50] Batch 300/938 D Loss: 0.6428 G Loss: 1.0339\n",
      "Epoch [7/50] Batch 400/938 D Loss: 0.5504 G Loss: 1.0735\n",
      "Epoch [7/50] Batch 500/938 D Loss: 0.6322 G Loss: 1.0782\n",
      "Epoch [7/50] Batch 600/938 D Loss: 0.6348 G Loss: 1.0733\n",
      "Epoch [7/50] Batch 700/938 D Loss: 0.5963 G Loss: 1.0533\n",
      "Epoch [7/50] Batch 800/938 D Loss: 0.6168 G Loss: 1.2631\n",
      "Epoch [7/50] Batch 900/938 D Loss: 0.6307 G Loss: 0.9530\n",
      "Epoch [7/50] Final D Loss: 0.6698 Final G Loss: 1.4045\n",
      "Epoch [8/50] Batch 0/938 D Loss: 0.5936 G Loss: 1.1471\n",
      "Epoch [8/50] Batch 100/938 D Loss: 0.6824 G Loss: 1.1261\n",
      "Epoch [8/50] Batch 200/938 D Loss: 0.5930 G Loss: 1.1005\n",
      "Epoch [8/50] Batch 300/938 D Loss: 0.6031 G Loss: 1.2596\n",
      "Epoch [8/50] Batch 400/938 D Loss: 0.5913 G Loss: 0.9275\n",
      "Epoch [8/50] Batch 500/938 D Loss: 0.5826 G Loss: 0.9461\n",
      "Epoch [8/50] Batch 600/938 D Loss: 0.6093 G Loss: 1.0797\n",
      "Epoch [8/50] Batch 700/938 D Loss: 0.5862 G Loss: 0.8935\n",
      "Epoch [8/50] Batch 800/938 D Loss: 0.5406 G Loss: 1.2112\n",
      "Epoch [8/50] Batch 900/938 D Loss: 0.5798 G Loss: 1.0872\n",
      "Epoch [8/50] Final D Loss: 0.6080 Final G Loss: 1.0478\n",
      "Epoch [9/50] Batch 0/938 D Loss: 0.6466 G Loss: 0.8681\n",
      "Epoch [9/50] Batch 100/938 D Loss: 0.6366 G Loss: 0.9299\n",
      "Epoch [9/50] Batch 200/938 D Loss: 0.6516 G Loss: 1.1460\n",
      "Epoch [9/50] Batch 300/938 D Loss: 0.6039 G Loss: 0.9740\n",
      "Epoch [9/50] Batch 400/938 D Loss: 0.5918 G Loss: 1.0131\n",
      "Epoch [9/50] Batch 500/938 D Loss: 0.6411 G Loss: 1.0015\n",
      "Epoch [9/50] Batch 600/938 D Loss: 0.6210 G Loss: 1.0113\n",
      "Epoch [9/50] Batch 700/938 D Loss: 0.6210 G Loss: 0.9902\n",
      "Epoch [9/50] Batch 800/938 D Loss: 0.6516 G Loss: 0.8259\n",
      "Epoch [9/50] Batch 900/938 D Loss: 0.6033 G Loss: 1.0055\n",
      "Epoch [9/50] Final D Loss: 0.6331 Final G Loss: 0.9738\n",
      "Epoch [10/50] Batch 0/938 D Loss: 0.5891 G Loss: 0.9563\n",
      "Epoch [10/50] Batch 100/938 D Loss: 0.5994 G Loss: 0.9113\n",
      "Epoch [10/50] Batch 200/938 D Loss: 0.6168 G Loss: 0.9626\n",
      "Epoch [10/50] Batch 300/938 D Loss: 0.6195 G Loss: 1.0488\n",
      "Epoch [10/50] Batch 400/938 D Loss: 0.7119 G Loss: 1.0435\n",
      "Epoch [10/50] Batch 500/938 D Loss: 0.6746 G Loss: 1.0249\n",
      "Epoch [10/50] Batch 600/938 D Loss: 0.6474 G Loss: 0.9900\n",
      "Epoch [10/50] Batch 700/938 D Loss: 0.6184 G Loss: 0.8789\n",
      "Epoch [10/50] Batch 800/938 D Loss: 0.6005 G Loss: 0.8840\n",
      "Epoch [10/50] Batch 900/938 D Loss: 0.6215 G Loss: 1.0534\n",
      "Epoch [10/50] Final D Loss: 0.6883 Final G Loss: 0.8811\n",
      "Epoch [11/50] Batch 0/938 D Loss: 0.6322 G Loss: 0.8551\n",
      "Epoch [11/50] Batch 100/938 D Loss: 0.6999 G Loss: 0.9080\n",
      "Epoch [11/50] Batch 200/938 D Loss: 0.6326 G Loss: 0.9920\n",
      "Epoch [11/50] Batch 300/938 D Loss: 0.6279 G Loss: 0.9457\n",
      "Epoch [11/50] Batch 400/938 D Loss: 0.6357 G Loss: 0.8793\n",
      "Epoch [11/50] Batch 500/938 D Loss: 0.6214 G Loss: 1.0074\n",
      "Epoch [11/50] Batch 600/938 D Loss: 0.6475 G Loss: 0.9629\n",
      "Epoch [11/50] Batch 700/938 D Loss: 0.6473 G Loss: 0.9284\n",
      "Epoch [11/50] Batch 800/938 D Loss: 0.6562 G Loss: 1.0770\n",
      "Epoch [11/50] Batch 900/938 D Loss: 0.6686 G Loss: 1.0133\n",
      "Epoch [11/50] Final D Loss: 0.6040 Final G Loss: 0.9796\n",
      "Epoch [12/50] Batch 0/938 D Loss: 0.6447 G Loss: 1.0048\n",
      "Epoch [12/50] Batch 100/938 D Loss: 0.6219 G Loss: 1.0546\n",
      "Epoch [12/50] Batch 200/938 D Loss: 0.5925 G Loss: 0.9445\n",
      "Epoch [12/50] Batch 300/938 D Loss: 0.6725 G Loss: 0.9315\n",
      "Epoch [12/50] Batch 400/938 D Loss: 0.6265 G Loss: 0.9402\n",
      "Epoch [12/50] Batch 500/938 D Loss: 0.6469 G Loss: 0.8086\n",
      "Epoch [12/50] Batch 600/938 D Loss: 0.6172 G Loss: 0.9020\n",
      "Epoch [12/50] Batch 700/938 D Loss: 0.6338 G Loss: 0.9965\n",
      "Epoch [12/50] Batch 800/938 D Loss: 0.6420 G Loss: 0.8948\n",
      "Epoch [12/50] Batch 900/938 D Loss: 0.6739 G Loss: 1.0388\n",
      "Epoch [12/50] Final D Loss: 0.6515 Final G Loss: 0.9790\n",
      "Epoch [13/50] Batch 0/938 D Loss: 0.6787 G Loss: 0.9495\n",
      "Epoch [13/50] Batch 100/938 D Loss: 0.6683 G Loss: 0.9352\n",
      "Epoch [13/50] Batch 200/938 D Loss: 0.6796 G Loss: 0.9096\n",
      "Epoch [13/50] Batch 300/938 D Loss: 0.6180 G Loss: 0.9953\n",
      "Epoch [13/50] Batch 400/938 D Loss: 0.6394 G Loss: 0.8896\n",
      "Epoch [13/50] Batch 500/938 D Loss: 0.6224 G Loss: 0.9139\n",
      "Epoch [13/50] Batch 600/938 D Loss: 0.6498 G Loss: 0.9028\n",
      "Epoch [13/50] Batch 700/938 D Loss: 0.6629 G Loss: 0.8679\n",
      "Epoch [13/50] Batch 800/938 D Loss: 0.6591 G Loss: 0.8903\n",
      "Epoch [13/50] Batch 900/938 D Loss: 0.6700 G Loss: 0.9942\n",
      "Epoch [13/50] Final D Loss: 0.6813 Final G Loss: 1.0328\n",
      "Epoch [14/50] Batch 0/938 D Loss: 0.6398 G Loss: 0.9101\n",
      "Epoch [14/50] Batch 100/938 D Loss: 0.7437 G Loss: 0.9965\n",
      "Epoch [14/50] Batch 200/938 D Loss: 0.6488 G Loss: 0.9105\n",
      "Epoch [14/50] Batch 300/938 D Loss: 0.6630 G Loss: 0.9639\n",
      "Epoch [14/50] Batch 400/938 D Loss: 0.6062 G Loss: 0.8697\n",
      "Epoch [14/50] Batch 500/938 D Loss: 0.6069 G Loss: 0.8177\n",
      "Epoch [14/50] Batch 600/938 D Loss: 0.6436 G Loss: 0.8459\n",
      "Epoch [14/50] Batch 700/938 D Loss: 0.6314 G Loss: 0.9763\n",
      "Epoch [14/50] Batch 800/938 D Loss: 0.6544 G Loss: 0.9590\n",
      "Epoch [14/50] Batch 900/938 D Loss: 0.6321 G Loss: 0.7813\n",
      "Epoch [14/50] Final D Loss: 0.6325 Final G Loss: 0.9311\n",
      "Epoch [15/50] Batch 0/938 D Loss: 0.6538 G Loss: 0.8841\n",
      "Epoch [15/50] Batch 100/938 D Loss: 0.6604 G Loss: 0.9273\n",
      "Epoch [15/50] Batch 200/938 D Loss: 0.6313 G Loss: 0.8469\n",
      "Epoch [15/50] Batch 300/938 D Loss: 0.6398 G Loss: 0.8158\n",
      "Epoch [15/50] Batch 400/938 D Loss: 0.6426 G Loss: 0.9406\n",
      "Epoch [15/50] Batch 500/938 D Loss: 0.6054 G Loss: 0.8529\n",
      "Epoch [15/50] Batch 600/938 D Loss: 0.6533 G Loss: 0.8552\n",
      "Epoch [15/50] Batch 700/938 D Loss: 0.6385 G Loss: 0.8967\n",
      "Epoch [15/50] Batch 800/938 D Loss: 0.6027 G Loss: 0.9154\n",
      "Epoch [15/50] Batch 900/938 D Loss: 0.6517 G Loss: 0.8837\n",
      "Epoch [15/50] Final D Loss: 0.7101 Final G Loss: 0.8717\n",
      "Epoch [16/50] Batch 0/938 D Loss: 0.6173 G Loss: 0.8816\n",
      "Epoch [16/50] Batch 100/938 D Loss: 0.6397 G Loss: 0.9245\n",
      "Epoch [16/50] Batch 200/938 D Loss: 0.6460 G Loss: 0.9312\n",
      "Epoch [16/50] Batch 300/938 D Loss: 0.6725 G Loss: 0.9331\n",
      "Epoch [16/50] Batch 400/938 D Loss: 0.6543 G Loss: 0.8916\n",
      "Epoch [16/50] Batch 500/938 D Loss: 0.6399 G Loss: 0.8971\n",
      "Epoch [16/50] Batch 600/938 D Loss: 0.6542 G Loss: 0.8447\n",
      "Epoch [16/50] Batch 700/938 D Loss: 0.6281 G Loss: 0.8611\n",
      "Epoch [16/50] Batch 800/938 D Loss: 0.6841 G Loss: 0.7915\n",
      "Epoch [16/50] Batch 900/938 D Loss: 0.7188 G Loss: 0.8531\n",
      "Epoch [16/50] Final D Loss: 0.6281 Final G Loss: 0.9469\n",
      "Epoch [17/50] Batch 0/938 D Loss: 0.6434 G Loss: 0.9287\n",
      "Epoch [17/50] Batch 100/938 D Loss: 0.6583 G Loss: 0.8670\n",
      "Epoch [17/50] Batch 200/938 D Loss: 0.6905 G Loss: 0.8615\n",
      "Epoch [17/50] Batch 300/938 D Loss: 0.6172 G Loss: 0.9035\n",
      "Epoch [17/50] Batch 400/938 D Loss: 0.6490 G Loss: 0.9008\n",
      "Epoch [17/50] Batch 500/938 D Loss: 0.6200 G Loss: 0.8887\n",
      "Epoch [17/50] Batch 600/938 D Loss: 0.6910 G Loss: 0.9001\n",
      "Epoch [17/50] Batch 700/938 D Loss: 0.7248 G Loss: 0.7939\n",
      "Epoch [17/50] Batch 800/938 D Loss: 0.6290 G Loss: 0.8654\n",
      "Epoch [17/50] Batch 900/938 D Loss: 0.6517 G Loss: 0.8989\n",
      "Epoch [17/50] Final D Loss: 0.6342 Final G Loss: 0.8293\n",
      "Epoch [18/50] Batch 0/938 D Loss: 0.6969 G Loss: 0.8724\n",
      "Epoch [18/50] Batch 100/938 D Loss: 0.6359 G Loss: 0.8598\n",
      "Epoch [18/50] Batch 200/938 D Loss: 0.6452 G Loss: 0.8720\n",
      "Epoch [18/50] Batch 300/938 D Loss: 0.6152 G Loss: 0.9381\n",
      "Epoch [18/50] Batch 400/938 D Loss: 0.6531 G Loss: 0.8819\n",
      "Epoch [18/50] Batch 500/938 D Loss: 0.6531 G Loss: 0.7649\n",
      "Epoch [18/50] Batch 600/938 D Loss: 0.6728 G Loss: 0.8379\n",
      "Epoch [18/50] Batch 700/938 D Loss: 0.6433 G Loss: 0.8073\n",
      "Epoch [18/50] Batch 800/938 D Loss: 0.6453 G Loss: 0.8225\n",
      "Epoch [18/50] Batch 900/938 D Loss: 0.6372 G Loss: 0.8315\n",
      "Epoch [18/50] Final D Loss: 0.6198 Final G Loss: 0.8213\n",
      "Epoch [19/50] Batch 0/938 D Loss: 0.6874 G Loss: 0.8451\n",
      "Epoch [19/50] Batch 100/938 D Loss: 0.6249 G Loss: 0.8628\n",
      "Epoch [19/50] Batch 200/938 D Loss: 0.6427 G Loss: 0.8376\n",
      "Epoch [19/50] Batch 300/938 D Loss: 0.7085 G Loss: 0.8255\n",
      "Epoch [19/50] Batch 400/938 D Loss: 0.6796 G Loss: 0.9472\n",
      "Epoch [19/50] Batch 500/938 D Loss: 0.6793 G Loss: 0.8718\n",
      "Epoch [19/50] Batch 600/938 D Loss: 0.7163 G Loss: 0.9049\n",
      "Epoch [19/50] Batch 700/938 D Loss: 0.6359 G Loss: 0.9261\n",
      "Epoch [19/50] Batch 800/938 D Loss: 0.6447 G Loss: 0.8675\n",
      "Epoch [19/50] Batch 900/938 D Loss: 0.6395 G Loss: 0.8841\n",
      "Epoch [19/50] Final D Loss: 0.7066 Final G Loss: 0.9600\n",
      "Epoch [20/50] Batch 0/938 D Loss: 0.6358 G Loss: 0.8349\n",
      "Epoch [20/50] Batch 100/938 D Loss: 0.6760 G Loss: 0.8490\n",
      "Epoch [20/50] Batch 200/938 D Loss: 0.6673 G Loss: 0.8239\n",
      "Epoch [20/50] Batch 300/938 D Loss: 0.6924 G Loss: 0.9443\n",
      "Epoch [20/50] Batch 400/938 D Loss: 0.6481 G Loss: 0.8499\n",
      "Epoch [20/50] Batch 500/938 D Loss: 0.6696 G Loss: 0.8580\n",
      "Epoch [20/50] Batch 600/938 D Loss: 0.6591 G Loss: 0.8904\n",
      "Epoch [20/50] Batch 700/938 D Loss: 0.6632 G Loss: 0.8135\n",
      "Epoch [20/50] Batch 800/938 D Loss: 0.6407 G Loss: 0.8326\n",
      "Epoch [20/50] Batch 900/938 D Loss: 0.6218 G Loss: 0.8646\n",
      "Epoch [20/50] Final D Loss: 0.6513 Final G Loss: 0.9097\n",
      "Epoch [21/50] Batch 0/938 D Loss: 0.6360 G Loss: 0.8491\n",
      "Epoch [21/50] Batch 100/938 D Loss: 0.6718 G Loss: 0.8146\n",
      "Epoch [21/50] Batch 200/938 D Loss: 0.6079 G Loss: 0.9570\n",
      "Epoch [21/50] Batch 300/938 D Loss: 0.6363 G Loss: 0.7887\n",
      "Epoch [21/50] Batch 400/938 D Loss: 0.6595 G Loss: 0.8234\n",
      "Epoch [21/50] Batch 500/938 D Loss: 0.6258 G Loss: 0.8656\n",
      "Epoch [21/50] Batch 600/938 D Loss: 0.6177 G Loss: 0.9025\n",
      "Epoch [21/50] Batch 700/938 D Loss: 0.6397 G Loss: 0.9285\n",
      "Epoch [21/50] Batch 800/938 D Loss: 0.6983 G Loss: 0.8258\n",
      "Epoch [21/50] Batch 900/938 D Loss: 0.7006 G Loss: 0.9061\n",
      "Epoch [21/50] Final D Loss: 0.6061 Final G Loss: 0.8557\n",
      "Epoch [22/50] Batch 0/938 D Loss: 0.6334 G Loss: 0.8568\n",
      "Epoch [22/50] Batch 100/938 D Loss: 0.6874 G Loss: 0.9218\n",
      "Epoch [22/50] Batch 200/938 D Loss: 0.6870 G Loss: 0.8571\n",
      "Epoch [22/50] Batch 300/938 D Loss: 0.6722 G Loss: 0.8407\n",
      "Epoch [22/50] Batch 400/938 D Loss: 0.6445 G Loss: 0.8439\n",
      "Epoch [22/50] Batch 500/938 D Loss: 0.6666 G Loss: 0.8725\n",
      "Epoch [22/50] Batch 600/938 D Loss: 0.6973 G Loss: 0.8427\n",
      "Epoch [22/50] Batch 700/938 D Loss: 0.6602 G Loss: 0.9060\n",
      "Epoch [22/50] Batch 800/938 D Loss: 0.6535 G Loss: 0.8275\n",
      "Epoch [22/50] Batch 900/938 D Loss: 0.6486 G Loss: 0.8264\n",
      "Epoch [22/50] Final D Loss: 0.6659 Final G Loss: 0.8958\n",
      "Epoch [23/50] Batch 0/938 D Loss: 0.6530 G Loss: 0.8508\n",
      "Epoch [23/50] Batch 100/938 D Loss: 0.7036 G Loss: 0.8615\n",
      "Epoch [23/50] Batch 200/938 D Loss: 0.6519 G Loss: 0.8842\n",
      "Epoch [23/50] Batch 300/938 D Loss: 0.6423 G Loss: 0.8391\n",
      "Epoch [23/50] Batch 400/938 D Loss: 0.6387 G Loss: 0.8748\n",
      "Epoch [23/50] Batch 500/938 D Loss: 0.6708 G Loss: 0.8429\n",
      "Epoch [23/50] Batch 600/938 D Loss: 0.6872 G Loss: 0.8689\n",
      "Epoch [23/50] Batch 700/938 D Loss: 0.6029 G Loss: 0.8754\n",
      "Epoch [23/50] Batch 800/938 D Loss: 0.6796 G Loss: 0.8355\n",
      "Epoch [23/50] Batch 900/938 D Loss: 0.6958 G Loss: 0.8557\n",
      "Epoch [23/50] Final D Loss: 0.6572 Final G Loss: 0.7644\n",
      "Epoch [24/50] Batch 0/938 D Loss: 0.6931 G Loss: 0.8501\n",
      "Epoch [24/50] Batch 100/938 D Loss: 0.6561 G Loss: 0.9163\n",
      "Epoch [24/50] Batch 200/938 D Loss: 0.6480 G Loss: 0.8633\n",
      "Epoch [24/50] Batch 300/938 D Loss: 0.6811 G Loss: 0.7910\n",
      "Epoch [24/50] Batch 400/938 D Loss: 0.6457 G Loss: 0.7875\n",
      "Epoch [24/50] Batch 500/938 D Loss: 0.6292 G Loss: 0.8368\n",
      "Epoch [24/50] Batch 600/938 D Loss: 0.6370 G Loss: 0.8435\n",
      "Epoch [24/50] Batch 700/938 D Loss: 0.6761 G Loss: 0.7902\n",
      "Epoch [24/50] Batch 800/938 D Loss: 0.6845 G Loss: 0.8234\n",
      "Epoch [24/50] Batch 900/938 D Loss: 0.6664 G Loss: 0.8613\n",
      "Epoch [24/50] Final D Loss: 0.5978 Final G Loss: 0.9291\n",
      "Epoch [25/50] Batch 0/938 D Loss: 0.6828 G Loss: 0.8473\n",
      "Epoch [25/50] Batch 100/938 D Loss: 0.6370 G Loss: 0.9178\n",
      "Epoch [25/50] Batch 200/938 D Loss: 0.6633 G Loss: 0.8612\n",
      "Epoch [25/50] Batch 300/938 D Loss: 0.6826 G Loss: 0.8018\n",
      "Epoch [25/50] Batch 400/938 D Loss: 0.6288 G Loss: 0.7751\n",
      "Epoch [25/50] Batch 500/938 D Loss: 0.6573 G Loss: 0.8750\n",
      "Epoch [25/50] Batch 600/938 D Loss: 0.6378 G Loss: 0.8114\n",
      "Epoch [25/50] Batch 700/938 D Loss: 0.6413 G Loss: 0.8199\n",
      "Epoch [25/50] Batch 800/938 D Loss: 0.6917 G Loss: 0.8254\n",
      "Epoch [25/50] Batch 900/938 D Loss: 0.6910 G Loss: 0.8246\n",
      "Epoch [25/50] Final D Loss: 0.6300 Final G Loss: 0.9145\n",
      "Epoch [26/50] Batch 0/938 D Loss: 0.6876 G Loss: 0.8493\n",
      "Epoch [26/50] Batch 100/938 D Loss: 0.6634 G Loss: 0.8141\n",
      "Epoch [26/50] Batch 200/938 D Loss: 0.6753 G Loss: 0.8246\n",
      "Epoch [26/50] Batch 300/938 D Loss: 0.6397 G Loss: 0.8888\n",
      "Epoch [26/50] Batch 400/938 D Loss: 0.6768 G Loss: 0.8214\n",
      "Epoch [26/50] Batch 500/938 D Loss: 0.6399 G Loss: 0.8145\n",
      "Epoch [26/50] Batch 600/938 D Loss: 0.6669 G Loss: 0.8024\n",
      "Epoch [26/50] Batch 700/938 D Loss: 0.6468 G Loss: 0.8536\n",
      "Epoch [26/50] Batch 800/938 D Loss: 0.6315 G Loss: 0.8412\n",
      "Epoch [26/50] Batch 900/938 D Loss: 0.6578 G Loss: 0.8596\n",
      "Epoch [26/50] Final D Loss: 0.6540 Final G Loss: 0.8989\n",
      "Epoch [27/50] Batch 0/938 D Loss: 0.6764 G Loss: 0.9026\n",
      "Epoch [27/50] Batch 100/938 D Loss: 0.6678 G Loss: 0.7951\n",
      "Epoch [27/50] Batch 200/938 D Loss: 0.6435 G Loss: 0.9448\n",
      "Epoch [27/50] Batch 300/938 D Loss: 0.6539 G Loss: 0.8450\n",
      "Epoch [27/50] Batch 400/938 D Loss: 0.6691 G Loss: 0.8642\n",
      "Epoch [27/50] Batch 500/938 D Loss: 0.6589 G Loss: 0.8572\n",
      "Epoch [27/50] Batch 600/938 D Loss: 0.6623 G Loss: 0.8159\n",
      "Epoch [27/50] Batch 700/938 D Loss: 0.6261 G Loss: 0.8381\n",
      "Epoch [27/50] Batch 800/938 D Loss: 0.6620 G Loss: 0.8725\n",
      "Epoch [27/50] Batch 900/938 D Loss: 0.6705 G Loss: 0.8326\n",
      "Epoch [27/50] Final D Loss: 0.6403 Final G Loss: 0.8724\n",
      "Epoch [28/50] Batch 0/938 D Loss: 0.6235 G Loss: 0.8314\n",
      "Epoch [28/50] Batch 100/938 D Loss: 0.6972 G Loss: 0.9177\n",
      "Epoch [28/50] Batch 200/938 D Loss: 0.6501 G Loss: 0.8859\n",
      "Epoch [28/50] Batch 300/938 D Loss: 0.6584 G Loss: 0.8015\n",
      "Epoch [28/50] Batch 400/938 D Loss: 0.6561 G Loss: 0.7593\n",
      "Epoch [28/50] Batch 500/938 D Loss: 0.6214 G Loss: 0.8724\n",
      "Epoch [28/50] Batch 600/938 D Loss: 0.6583 G Loss: 0.7735\n",
      "Epoch [28/50] Batch 700/938 D Loss: 0.6105 G Loss: 0.8479\n",
      "Epoch [28/50] Batch 800/938 D Loss: 0.7123 G Loss: 0.9248\n",
      "Epoch [28/50] Batch 900/938 D Loss: 0.6604 G Loss: 0.7969\n",
      "Epoch [28/50] Final D Loss: 0.6536 Final G Loss: 0.8008\n",
      "Epoch [29/50] Batch 0/938 D Loss: 0.6534 G Loss: 0.8761\n",
      "Epoch [29/50] Batch 100/938 D Loss: 0.6349 G Loss: 0.9016\n",
      "Epoch [29/50] Batch 200/938 D Loss: 0.6922 G Loss: 0.8619\n",
      "Epoch [29/50] Batch 300/938 D Loss: 0.6547 G Loss: 0.8432\n",
      "Epoch [29/50] Batch 400/938 D Loss: 0.6608 G Loss: 0.8443\n",
      "Epoch [29/50] Batch 500/938 D Loss: 0.6445 G Loss: 0.8466\n",
      "Epoch [29/50] Batch 600/938 D Loss: 0.6656 G Loss: 0.8631\n",
      "Epoch [29/50] Batch 700/938 D Loss: 0.6472 G Loss: 0.7693\n",
      "Epoch [29/50] Batch 800/938 D Loss: 0.6598 G Loss: 0.8607\n",
      "Epoch [29/50] Batch 900/938 D Loss: 0.6325 G Loss: 0.8725\n",
      "Epoch [29/50] Final D Loss: 0.7065 Final G Loss: 0.8343\n",
      "Epoch [30/50] Batch 0/938 D Loss: 0.6685 G Loss: 0.8444\n",
      "Epoch [30/50] Batch 100/938 D Loss: 0.6494 G Loss: 0.7617\n",
      "Epoch [30/50] Batch 200/938 D Loss: 0.6487 G Loss: 0.8166\n",
      "Epoch [30/50] Batch 300/938 D Loss: 0.6537 G Loss: 0.8614\n",
      "Epoch [30/50] Batch 400/938 D Loss: 0.6661 G Loss: 0.9476\n",
      "Epoch [30/50] Batch 500/938 D Loss: 0.6175 G Loss: 0.8991\n",
      "Epoch [30/50] Batch 600/938 D Loss: 0.6795 G Loss: 0.9246\n",
      "Epoch [30/50] Batch 700/938 D Loss: 0.6644 G Loss: 0.8010\n",
      "Epoch [30/50] Batch 800/938 D Loss: 0.6745 G Loss: 0.7791\n",
      "Epoch [30/50] Batch 900/938 D Loss: 0.6390 G Loss: 0.7832\n",
      "Epoch [30/50] Final D Loss: 0.6729 Final G Loss: 0.8398\n",
      "Epoch [31/50] Batch 0/938 D Loss: 0.6443 G Loss: 0.9017\n",
      "Epoch [31/50] Batch 100/938 D Loss: 0.6429 G Loss: 0.7417\n",
      "Epoch [31/50] Batch 200/938 D Loss: 0.6494 G Loss: 0.8514\n",
      "Epoch [31/50] Batch 300/938 D Loss: 0.6545 G Loss: 0.7831\n",
      "Epoch [31/50] Batch 400/938 D Loss: 0.6591 G Loss: 0.8076\n",
      "Epoch [31/50] Batch 500/938 D Loss: 0.6480 G Loss: 0.8413\n",
      "Epoch [31/50] Batch 600/938 D Loss: 0.6467 G Loss: 0.8805\n",
      "Epoch [31/50] Batch 700/938 D Loss: 0.6448 G Loss: 0.9165\n",
      "Epoch [31/50] Batch 800/938 D Loss: 0.6953 G Loss: 0.8051\n",
      "Epoch [31/50] Batch 900/938 D Loss: 0.6604 G Loss: 0.7991\n",
      "Epoch [31/50] Final D Loss: 0.6849 Final G Loss: 0.8924\n",
      "Epoch [32/50] Batch 0/938 D Loss: 0.6626 G Loss: 0.7595\n",
      "Epoch [32/50] Batch 100/938 D Loss: 0.6568 G Loss: 0.8697\n",
      "Epoch [32/50] Batch 200/938 D Loss: 0.6750 G Loss: 0.8449\n",
      "Epoch [32/50] Batch 300/938 D Loss: 0.7184 G Loss: 0.8588\n",
      "Epoch [32/50] Batch 400/938 D Loss: 0.6472 G Loss: 0.7550\n",
      "Epoch [32/50] Batch 500/938 D Loss: 0.6826 G Loss: 0.7958\n",
      "Epoch [32/50] Batch 600/938 D Loss: 0.6248 G Loss: 0.8190\n",
      "Epoch [32/50] Batch 700/938 D Loss: 0.6515 G Loss: 0.8242\n",
      "Epoch [32/50] Batch 800/938 D Loss: 0.6454 G Loss: 0.8443\n",
      "Epoch [32/50] Batch 900/938 D Loss: 0.6514 G Loss: 0.8147\n",
      "Epoch [32/50] Final D Loss: 0.6171 Final G Loss: 0.8466\n",
      "Epoch [33/50] Batch 0/938 D Loss: 0.6599 G Loss: 0.8724\n",
      "Epoch [33/50] Batch 100/938 D Loss: 0.6418 G Loss: 0.8118\n",
      "Epoch [33/50] Batch 200/938 D Loss: 0.6861 G Loss: 0.8808\n",
      "Epoch [33/50] Batch 300/938 D Loss: 0.6477 G Loss: 0.8261\n",
      "Epoch [33/50] Batch 400/938 D Loss: 0.6843 G Loss: 0.7320\n",
      "Epoch [33/50] Batch 500/938 D Loss: 0.6342 G Loss: 0.8622\n",
      "Epoch [33/50] Batch 600/938 D Loss: 0.6413 G Loss: 0.9465\n",
      "Epoch [33/50] Batch 700/938 D Loss: 0.6365 G Loss: 0.8385\n",
      "Epoch [33/50] Batch 800/938 D Loss: 0.6817 G Loss: 0.8215\n",
      "Epoch [33/50] Batch 900/938 D Loss: 0.6346 G Loss: 0.7814\n",
      "Epoch [33/50] Final D Loss: 0.6788 Final G Loss: 0.8777\n",
      "Epoch [34/50] Batch 0/938 D Loss: 0.6374 G Loss: 0.8296\n",
      "Epoch [34/50] Batch 100/938 D Loss: 0.6173 G Loss: 0.7969\n",
      "Epoch [34/50] Batch 200/938 D Loss: 0.6460 G Loss: 0.8030\n",
      "Epoch [34/50] Batch 300/938 D Loss: 0.6366 G Loss: 0.8130\n",
      "Epoch [34/50] Batch 400/938 D Loss: 0.6339 G Loss: 0.7978\n",
      "Epoch [34/50] Batch 500/938 D Loss: 0.6403 G Loss: 0.8223\n",
      "Epoch [34/50] Batch 600/938 D Loss: 0.6492 G Loss: 0.8243\n",
      "Epoch [34/50] Batch 700/938 D Loss: 0.7160 G Loss: 0.8637\n",
      "Epoch [34/50] Batch 800/938 D Loss: 0.6429 G Loss: 0.8661\n",
      "Epoch [34/50] Batch 900/938 D Loss: 0.6660 G Loss: 0.7997\n",
      "Epoch [34/50] Final D Loss: 0.6153 Final G Loss: 0.8495\n",
      "Epoch [35/50] Batch 0/938 D Loss: 0.6344 G Loss: 0.8180\n",
      "Epoch [35/50] Batch 100/938 D Loss: 0.6263 G Loss: 0.9008\n",
      "Epoch [35/50] Batch 200/938 D Loss: 0.6142 G Loss: 0.9333\n",
      "Epoch [35/50] Batch 300/938 D Loss: 0.7010 G Loss: 0.9365\n",
      "Epoch [35/50] Batch 400/938 D Loss: 0.6552 G Loss: 0.8034\n",
      "Epoch [35/50] Batch 500/938 D Loss: 0.6674 G Loss: 0.8760\n",
      "Epoch [35/50] Batch 600/938 D Loss: 0.6913 G Loss: 0.8731\n",
      "Epoch [35/50] Batch 700/938 D Loss: 0.6178 G Loss: 0.8775\n",
      "Epoch [35/50] Batch 800/938 D Loss: 0.6457 G Loss: 0.8093\n",
      "Epoch [35/50] Batch 900/938 D Loss: 0.6641 G Loss: 0.8350\n",
      "Epoch [35/50] Final D Loss: 0.6332 Final G Loss: 1.0166\n",
      "Epoch [36/50] Batch 0/938 D Loss: 0.6460 G Loss: 0.9159\n",
      "Epoch [36/50] Batch 100/938 D Loss: 0.6730 G Loss: 0.8088\n",
      "Epoch [36/50] Batch 200/938 D Loss: 0.6650 G Loss: 0.8142\n",
      "Epoch [36/50] Batch 300/938 D Loss: 0.6462 G Loss: 0.8577\n",
      "Epoch [36/50] Batch 400/938 D Loss: 0.6641 G Loss: 0.7884\n",
      "Epoch [36/50] Batch 500/938 D Loss: 0.6152 G Loss: 0.8569\n",
      "Epoch [36/50] Batch 600/938 D Loss: 0.6520 G Loss: 0.8360\n",
      "Epoch [36/50] Batch 700/938 D Loss: 0.6627 G Loss: 0.8184\n",
      "Epoch [36/50] Batch 800/938 D Loss: 0.6792 G Loss: 0.8101\n",
      "Epoch [36/50] Batch 900/938 D Loss: 0.6442 G Loss: 0.8128\n",
      "Epoch [36/50] Final D Loss: 0.6823 Final G Loss: 0.8785\n",
      "Epoch [37/50] Batch 0/938 D Loss: 0.6372 G Loss: 0.8071\n",
      "Epoch [37/50] Batch 100/938 D Loss: 0.6302 G Loss: 0.9058\n",
      "Epoch [37/50] Batch 200/938 D Loss: 0.6480 G Loss: 0.9269\n",
      "Epoch [37/50] Batch 300/938 D Loss: 0.6714 G Loss: 0.8494\n",
      "Epoch [37/50] Batch 400/938 D Loss: 0.6943 G Loss: 0.8684\n",
      "Epoch [37/50] Batch 500/938 D Loss: 0.6392 G Loss: 0.9031\n",
      "Epoch [37/50] Batch 600/938 D Loss: 0.6657 G Loss: 0.8669\n",
      "Epoch [37/50] Batch 700/938 D Loss: 0.6801 G Loss: 0.7833\n",
      "Epoch [37/50] Batch 800/938 D Loss: 0.6626 G Loss: 0.8725\n",
      "Epoch [37/50] Batch 900/938 D Loss: 0.6500 G Loss: 0.8633\n",
      "Epoch [37/50] Final D Loss: 0.6574 Final G Loss: 0.8398\n",
      "Epoch [38/50] Batch 0/938 D Loss: 0.6601 G Loss: 0.8474\n",
      "Epoch [38/50] Batch 100/938 D Loss: 0.6160 G Loss: 0.8645\n",
      "Epoch [38/50] Batch 200/938 D Loss: 0.7467 G Loss: 0.8952\n",
      "Epoch [38/50] Batch 300/938 D Loss: 0.6031 G Loss: 0.8321\n",
      "Epoch [38/50] Batch 400/938 D Loss: 0.6569 G Loss: 0.8344\n",
      "Epoch [38/50] Batch 500/938 D Loss: 0.6694 G Loss: 0.8123\n",
      "Epoch [38/50] Batch 600/938 D Loss: 0.6257 G Loss: 0.8942\n",
      "Epoch [38/50] Batch 700/938 D Loss: 0.6439 G Loss: 0.8487\n",
      "Epoch [38/50] Batch 800/938 D Loss: 0.6301 G Loss: 0.7452\n",
      "Epoch [38/50] Batch 900/938 D Loss: 0.6622 G Loss: 0.9163\n",
      "Epoch [38/50] Final D Loss: 0.6102 Final G Loss: 0.9644\n",
      "Epoch [39/50] Batch 0/938 D Loss: 0.6227 G Loss: 0.9018\n",
      "Epoch [39/50] Batch 100/938 D Loss: 0.6839 G Loss: 0.7887\n",
      "Epoch [39/50] Batch 200/938 D Loss: 0.6474 G Loss: 0.8198\n",
      "Epoch [39/50] Batch 300/938 D Loss: 0.6381 G Loss: 0.8825\n",
      "Epoch [39/50] Batch 400/938 D Loss: 0.6699 G Loss: 0.8553\n",
      "Epoch [39/50] Batch 500/938 D Loss: 0.6650 G Loss: 0.8778\n",
      "Epoch [39/50] Batch 600/938 D Loss: 0.6714 G Loss: 0.8307\n",
      "Epoch [39/50] Batch 700/938 D Loss: 0.6086 G Loss: 0.8384\n",
      "Epoch [39/50] Batch 800/938 D Loss: 0.6802 G Loss: 0.8629\n",
      "Epoch [39/50] Batch 900/938 D Loss: 0.6461 G Loss: 0.8959\n",
      "Epoch [39/50] Final D Loss: 0.6784 Final G Loss: 0.9781\n",
      "Epoch [40/50] Batch 0/938 D Loss: 0.6640 G Loss: 0.8574\n",
      "Epoch [40/50] Batch 100/938 D Loss: 0.6742 G Loss: 0.8418\n",
      "Epoch [40/50] Batch 200/938 D Loss: 0.6251 G Loss: 0.8763\n",
      "Epoch [40/50] Batch 300/938 D Loss: 0.6406 G Loss: 0.8880\n",
      "Epoch [40/50] Batch 400/938 D Loss: 0.6903 G Loss: 0.7879\n",
      "Epoch [40/50] Batch 500/938 D Loss: 0.6327 G Loss: 0.8294\n",
      "Epoch [40/50] Batch 600/938 D Loss: 0.6519 G Loss: 0.9080\n",
      "Epoch [40/50] Batch 700/938 D Loss: 0.6672 G Loss: 0.7856\n",
      "Epoch [40/50] Batch 800/938 D Loss: 0.6707 G Loss: 0.8727\n",
      "Epoch [40/50] Batch 900/938 D Loss: 0.6305 G Loss: 0.8332\n",
      "Epoch [40/50] Final D Loss: 0.6742 Final G Loss: 0.8436\n",
      "Epoch [41/50] Batch 0/938 D Loss: 0.6676 G Loss: 0.8342\n",
      "Epoch [41/50] Batch 100/938 D Loss: 0.6435 G Loss: 0.8154\n",
      "Epoch [41/50] Batch 200/938 D Loss: 0.6295 G Loss: 0.8643\n",
      "Epoch [41/50] Batch 300/938 D Loss: 0.6580 G Loss: 0.8301\n",
      "Epoch [41/50] Batch 400/938 D Loss: 0.6643 G Loss: 0.8930\n",
      "Epoch [41/50] Batch 500/938 D Loss: 0.6433 G Loss: 0.8072\n",
      "Epoch [41/50] Batch 600/938 D Loss: 0.6172 G Loss: 0.8229\n",
      "Epoch [41/50] Batch 700/938 D Loss: 0.6538 G Loss: 0.8107\n",
      "Epoch [41/50] Batch 800/938 D Loss: 0.6716 G Loss: 0.8097\n",
      "Epoch [41/50] Batch 900/938 D Loss: 0.6569 G Loss: 0.8834\n",
      "Epoch [41/50] Final D Loss: 0.6515 Final G Loss: 0.9674\n",
      "Epoch [42/50] Batch 0/938 D Loss: 0.6389 G Loss: 0.9098\n",
      "Epoch [42/50] Batch 100/938 D Loss: 0.6341 G Loss: 0.8249\n",
      "Epoch [42/50] Batch 200/938 D Loss: 0.6534 G Loss: 0.9065\n",
      "Epoch [42/50] Batch 300/938 D Loss: 0.6638 G Loss: 0.8573\n",
      "Epoch [42/50] Batch 400/938 D Loss: 0.6764 G Loss: 0.8257\n",
      "Epoch [42/50] Batch 500/938 D Loss: 0.6341 G Loss: 0.7761\n",
      "Epoch [42/50] Batch 600/938 D Loss: 0.6568 G Loss: 0.8458\n",
      "Epoch [42/50] Batch 700/938 D Loss: 0.6499 G Loss: 0.8664\n",
      "Epoch [42/50] Batch 800/938 D Loss: 0.6963 G Loss: 0.8106\n",
      "Epoch [42/50] Batch 900/938 D Loss: 0.6829 G Loss: 0.9151\n",
      "Epoch [42/50] Final D Loss: 0.6704 Final G Loss: 0.9220\n",
      "Epoch [43/50] Batch 0/938 D Loss: 0.6110 G Loss: 0.8474\n",
      "Epoch [43/50] Batch 100/938 D Loss: 0.6167 G Loss: 0.9681\n",
      "Epoch [43/50] Batch 200/938 D Loss: 0.6443 G Loss: 0.8411\n",
      "Epoch [43/50] Batch 300/938 D Loss: 0.6420 G Loss: 0.8903\n",
      "Epoch [43/50] Batch 400/938 D Loss: 0.6624 G Loss: 0.8614\n",
      "Epoch [43/50] Batch 500/938 D Loss: 0.6215 G Loss: 0.8871\n",
      "Epoch [43/50] Batch 600/938 D Loss: 0.6169 G Loss: 0.8111\n",
      "Epoch [43/50] Batch 700/938 D Loss: 0.6714 G Loss: 0.8412\n",
      "Epoch [43/50] Batch 800/938 D Loss: 0.6791 G Loss: 0.9226\n",
      "Epoch [43/50] Batch 900/938 D Loss: 0.6509 G Loss: 0.8296\n",
      "Epoch [43/50] Final D Loss: 0.6605 Final G Loss: 0.8504\n",
      "Epoch [44/50] Batch 0/938 D Loss: 0.6392 G Loss: 0.8200\n",
      "Epoch [44/50] Batch 100/938 D Loss: 0.6012 G Loss: 0.8543\n",
      "Epoch [44/50] Batch 200/938 D Loss: 0.6096 G Loss: 0.8021\n",
      "Epoch [44/50] Batch 300/938 D Loss: 0.6875 G Loss: 0.8566\n",
      "Epoch [44/50] Batch 400/938 D Loss: 0.6297 G Loss: 0.8830\n",
      "Epoch [44/50] Batch 500/938 D Loss: 0.6347 G Loss: 0.8735\n",
      "Epoch [44/50] Batch 600/938 D Loss: 0.6678 G Loss: 0.8536\n",
      "Epoch [44/50] Batch 700/938 D Loss: 0.6081 G Loss: 0.9078\n",
      "Epoch [44/50] Batch 800/938 D Loss: 0.7274 G Loss: 0.9193\n",
      "Epoch [44/50] Batch 900/938 D Loss: 0.6475 G Loss: 0.8273\n",
      "Epoch [44/50] Final D Loss: 0.6649 Final G Loss: 0.8493\n",
      "Epoch [45/50] Batch 0/938 D Loss: 0.6290 G Loss: 0.8378\n",
      "Epoch [45/50] Batch 100/938 D Loss: 0.6682 G Loss: 0.8496\n",
      "Epoch [45/50] Batch 200/938 D Loss: 0.6661 G Loss: 0.8155\n",
      "Epoch [45/50] Batch 300/938 D Loss: 0.6477 G Loss: 0.8728\n",
      "Epoch [45/50] Batch 400/938 D Loss: 0.5901 G Loss: 0.9467\n",
      "Epoch [45/50] Batch 500/938 D Loss: 0.6573 G Loss: 0.8556\n",
      "Epoch [45/50] Batch 600/938 D Loss: 0.7140 G Loss: 0.9293\n",
      "Epoch [45/50] Batch 700/938 D Loss: 0.6412 G Loss: 0.8302\n",
      "Epoch [45/50] Batch 800/938 D Loss: 0.6012 G Loss: 0.9136\n",
      "Epoch [45/50] Batch 900/938 D Loss: 0.6732 G Loss: 0.9497\n",
      "Epoch [45/50] Final D Loss: 0.6106 Final G Loss: 0.8650\n",
      "Epoch [46/50] Batch 0/938 D Loss: 0.6361 G Loss: 0.9514\n",
      "Epoch [46/50] Batch 100/938 D Loss: 0.6485 G Loss: 0.7347\n",
      "Epoch [46/50] Batch 200/938 D Loss: 0.6091 G Loss: 0.8863\n",
      "Epoch [46/50] Batch 300/938 D Loss: 0.6253 G Loss: 0.8705\n",
      "Epoch [46/50] Batch 400/938 D Loss: 0.6601 G Loss: 0.9242\n",
      "Epoch [46/50] Batch 500/938 D Loss: 0.6261 G Loss: 0.9515\n",
      "Epoch [46/50] Batch 600/938 D Loss: 0.6052 G Loss: 0.9626\n",
      "Epoch [46/50] Batch 700/938 D Loss: 0.6078 G Loss: 0.8463\n",
      "Epoch [46/50] Batch 800/938 D Loss: 0.6799 G Loss: 0.9065\n",
      "Epoch [46/50] Batch 900/938 D Loss: 0.6412 G Loss: 0.8891\n",
      "Epoch [46/50] Final D Loss: 0.6247 Final G Loss: 0.8298\n",
      "Epoch [47/50] Batch 0/938 D Loss: 0.6665 G Loss: 0.7667\n",
      "Epoch [47/50] Batch 100/938 D Loss: 0.6483 G Loss: 0.8651\n",
      "Epoch [47/50] Batch 200/938 D Loss: 0.5994 G Loss: 0.8512\n",
      "Epoch [47/50] Batch 300/938 D Loss: 0.6301 G Loss: 0.9052\n",
      "Epoch [47/50] Batch 400/938 D Loss: 0.6413 G Loss: 0.9464\n",
      "Epoch [47/50] Batch 500/938 D Loss: 0.6222 G Loss: 0.9195\n",
      "Epoch [47/50] Batch 600/938 D Loss: 0.6511 G Loss: 0.8785\n",
      "Epoch [47/50] Batch 700/938 D Loss: 0.6551 G Loss: 0.9354\n",
      "Epoch [47/50] Batch 800/938 D Loss: 0.6641 G Loss: 0.9656\n",
      "Epoch [47/50] Batch 900/938 D Loss: 0.6449 G Loss: 0.8594\n",
      "Epoch [47/50] Final D Loss: 0.6948 Final G Loss: 0.8094\n",
      "Epoch [48/50] Batch 0/938 D Loss: 0.6545 G Loss: 0.8276\n",
      "Epoch [48/50] Batch 100/938 D Loss: 0.6096 G Loss: 0.8269\n",
      "Epoch [48/50] Batch 200/938 D Loss: 0.6186 G Loss: 0.9176\n",
      "Epoch [48/50] Batch 300/938 D Loss: 0.6773 G Loss: 0.8817\n",
      "Epoch [48/50] Batch 400/938 D Loss: 0.6564 G Loss: 0.8064\n",
      "Epoch [48/50] Batch 500/938 D Loss: 0.6472 G Loss: 0.8449\n",
      "Epoch [48/50] Batch 600/938 D Loss: 0.6154 G Loss: 0.8346\n",
      "Epoch [48/50] Batch 700/938 D Loss: 0.6182 G Loss: 0.9261\n",
      "Epoch [48/50] Batch 800/938 D Loss: 0.6119 G Loss: 0.9695\n",
      "Epoch [48/50] Batch 900/938 D Loss: 0.6315 G Loss: 0.8378\n",
      "Epoch [48/50] Final D Loss: 0.6806 Final G Loss: 0.9185\n",
      "Epoch [49/50] Batch 0/938 D Loss: 0.6642 G Loss: 0.9000\n",
      "Epoch [49/50] Batch 100/938 D Loss: 0.6068 G Loss: 0.8508\n",
      "Epoch [49/50] Batch 200/938 D Loss: 0.6558 G Loss: 0.8221\n",
      "Epoch [49/50] Batch 300/938 D Loss: 0.6643 G Loss: 0.8453\n",
      "Epoch [49/50] Batch 400/938 D Loss: 0.6391 G Loss: 0.8761\n",
      "Epoch [49/50] Batch 500/938 D Loss: 0.6459 G Loss: 0.8958\n",
      "Epoch [49/50] Batch 600/938 D Loss: 0.6399 G Loss: 1.0263\n",
      "Epoch [49/50] Batch 700/938 D Loss: 0.5957 G Loss: 0.9145\n",
      "Epoch [49/50] Batch 800/938 D Loss: 0.6396 G Loss: 0.8770\n",
      "Epoch [49/50] Batch 900/938 D Loss: 0.6671 G Loss: 0.9011\n",
      "Epoch [49/50] Final D Loss: 0.6358 Final G Loss: 0.8414\n",
      "Epoch [50/50] Batch 0/938 D Loss: 0.6636 G Loss: 0.8583\n",
      "Epoch [50/50] Batch 100/938 D Loss: 0.6458 G Loss: 0.8806\n",
      "Epoch [50/50] Batch 200/938 D Loss: 0.6790 G Loss: 0.8183\n",
      "Epoch [50/50] Batch 300/938 D Loss: 0.5727 G Loss: 0.8741\n",
      "Epoch [50/50] Batch 400/938 D Loss: 0.6372 G Loss: 0.9167\n",
      "Epoch [50/50] Batch 500/938 D Loss: 0.6550 G Loss: 0.8338\n",
      "Epoch [50/50] Batch 600/938 D Loss: 0.6897 G Loss: 0.8921\n",
      "Epoch [50/50] Batch 700/938 D Loss: 0.6741 G Loss: 0.8731\n",
      "Epoch [50/50] Batch 800/938 D Loss: 0.6250 G Loss: 0.8717\n",
      "Epoch [50/50] Batch 900/938 D Loss: 0.6305 G Loss: 0.8504\n",
      "Epoch [50/50] Final D Loss: 0.6644 Final G Loss: 0.9360\n",
      "--- GAN Training Finished ---\n"
     ]
    }
   ],
   "source": [
    "transform_gan = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) #Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "#Use existing downloaded dataset if possible, otherwise download\n",
    "train_dataset_gan = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_gan)\n",
    "train_loader_gan = DataLoader(train_dataset_gan, batch_size=batch_size_gan, shuffle=True)\n",
    "\n",
    "# 3. Define the Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, image_size),\n",
    "            nn.Tanh() #NOTE: Output pixel values in [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1, 1, 28, 28) #Reshape to 1-channel 28x28 image (instead of image size 784)\n",
    "\n",
    "# 4. Define the Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(image_size, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid() # Output a probability [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, image_size) # Flatten the image\n",
    "        return self.main(input)\n",
    "\n",
    "# 5. Instantiate Models, Loss, and Optimizers\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "criterion_gan = nn.BCELoss() # Binary Cross-Entropy Loss for GANs\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=lr_gan, betas=(b1, b2))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=lr_gan, betas=(b1, b2))\n",
    "\n",
    "\n",
    "#6. Training Loop\n",
    "print(\"\\n--- Starting FashionMNIST GAN Training ---\")\n",
    "for epoch in range(epochs_gan):\n",
    "    for i, (imgs, _) in enumerate(train_loader_gan):\n",
    "        #Adversarial ground truths\n",
    "        valid = torch.ones(imgs.size(0), 1).to(device) #Real images are labeled 1\n",
    "        fake = torch.zeros(imgs.size(0), 1).to(device) #Fake images are labeled 0\n",
    "\n",
    "        #Configure input\n",
    "        real_imgs = imgs.to(device)\n",
    "\n",
    "\n",
    "        #Train Discriminator\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "\n",
    "        #Measure discriminator's ability to classify real images\n",
    "        z = torch.randn(imgs.size(0), latent_dim).to(device)\n",
    "        gen_imgs = generator(z) #Generate a batch of images\n",
    "\n",
    "        real_loss = criterion_gan(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion_gan(discriminator(gen_imgs.detach()), fake) #Example of how to detach generator to avoid training it (always good to do!)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "\n",
    "        #Train Generator\n",
    "\n",
    "        optimizer_g.zero_grad()\n",
    "\n",
    "        #Measure generator's ability to fool discriminator\n",
    "        g_loss = criterion_gan(discriminator(gen_imgs), valid)\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "\n",
    "        if i % 100 == 0: # Print every 100 batches\n",
    "            print(f\"Epoch [{epoch+1}/{epochs_gan}] Batch {i}/{len(train_loader_gan)} \"\n",
    "                  f\"D Loss: {d_loss.item():.4f} G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs_gan}] Final D Loss: {d_loss.item():.4f} Final G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "print(\"--- GAN Training Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "PBz-dbv_9jKx",
    "outputId": "f6f0e6fd-c6d9-4c54-ce9c-b8ec721cd5af"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "generator.eval() # Set generator to evaluation mode\n",
    "\n",
    "# Generate a random noise vector\n",
    "z = torch.randn(1, latent_dim).to(device)\n",
    "\n",
    "# Generate an image\n",
    "with torch.no_grad():\n",
    "    generated_image = generator(z).cpu()\n",
    "\n",
    "# Reshape and rescale the image for display\n",
    "img_np = generated_image.squeeze().numpy() # Remove batch dimension and channel dimension, convert to numpy\n",
    "img_np = (img_np + 1) / 2.0 # Rescale from [-1, 1] to [0, 1]\n",
    "\n",
    "# Display the image\n",
    "plt.imshow(img_np, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Generated Image')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
